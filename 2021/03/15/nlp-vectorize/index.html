<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="deep learning,nlp,one-hot,embedding,">










<meta name="description" content="本文章参考《python深度学习》 深度学习用于文本和序列深度学习用于自然语言处理是将模式识别应用于单词、 句子和段落，这与计算机视觉是将模式识别应用于像素大致相同。 文本向量化 与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它只能处理数值张量。 文本向量化（vectorize）是指将文本转换为数值张量的过程。它有多种实现方法。  将文本分割为单词，并将每个单词转换为一个向量。 将">
<meta name="keywords" content="deep learning,nlp,one-hot,embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习用于文本和序列">
<meta property="og:url" content="http://yoursite.com/2021/03/15/nlp-vectorize/index.html">
<meta property="og:site_name" content="Sunrise">
<meta property="og:description" content="本文章参考《python深度学习》 深度学习用于文本和序列深度学习用于自然语言处理是将模式识别应用于单词、 句子和段落，这与计算机视觉是将模式识别应用于像素大致相同。 文本向量化 与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它只能处理数值张量。 文本向量化（vectorize）是指将文本转换为数值张量的过程。它有多种实现方法。  将文本分割为单词，并将每个单词转换为一个向量。 将">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2021/03/15/nlp-vectorize/p1.png">
<meta property="og:image" content="http://yoursite.com/2021/03/15/nlp-vectorize/p2.png">
<meta property="og:updated_time" content="2022-10-15T04:46:44.685Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习用于文本和序列">
<meta name="twitter:description" content="本文章参考《python深度学习》 深度学习用于文本和序列深度学习用于自然语言处理是将模式识别应用于单词、 句子和段落，这与计算机视觉是将模式识别应用于像素大致相同。 文本向量化 与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它只能处理数值张量。 文本向量化（vectorize）是指将文本转换为数值张量的过程。它有多种实现方法。  将文本分割为单词，并将每个单词转换为一个向量。 将">
<meta name="twitter:image" content="http://yoursite.com/2021/03/15/nlp-vectorize/p1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/03/15/nlp-vectorize/">





  <title>深度学习用于文本和序列 | Sunrise</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sunrise</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">`长路漫漫，唯剑作伴`</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/15/nlp-vectorize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sunrise">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习用于文本和序列</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-15T20:19:54+08:00">
                2021-03-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/自然语言处理/" itemprop="url" rel="index">
                    <span itemprop="name">自然语言处理</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><em>本文章参考《python深度学习》</em></p>
<h2 id="深度学习用于文本和序列"><a href="#深度学习用于文本和序列" class="headerlink" title="深度学习用于文本和序列"></a>深度学习用于文本和序列</h2><p>深度学习用于自然语言处理是<strong>将模式识别应用于单词、 句子和段落</strong>，这与计算机视觉是将模式识别应用于像素大致相同。</p>
<h3 id="文本向量化"><a href="#文本向量化" class="headerlink" title="文本向量化"></a>文本向量化</h3><p> 与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它<strong>只能处理数值张量</strong>。 <strong>文本向量化（<code>vectorize</code>）</strong>是指将文本转换为数值张量的过程。它有多种实现方法。</p>
<ul>
<li>将文本分割为单词，并将每个单词转换为一个向量。</li>
<li>将文本分割为字符，并将每个字符转换为一个向量。 </li>
<li>提取单词或字符的 n-gram，并将每个 n-gram 转换为一个向量。n-gram 是多个连续单词 或字符的集合（n-gram 之间可重叠）。</li>
</ul>
<p>将文本分解而成的单元（单词、字符或 n-gram）叫作<strong>标记（token）</strong>，将文本分解成标记的过程叫作<strong>分词（<code>tokenization</code>）</strong>。英文分词相较中文分词简单一些，本身句子的空格就分割了不同的单词，而中文分词需要考虑一些歧义性等问题，比较复杂，<strong>后面会有单独的篇章介绍中文分词</strong>，这里是针对英文的文本向量化处理。</p>
<p>所有文本向量化过程都是应用某种分词方案，然后<strong>将数值向量与生成的标记相关联</strong>。这些向量组合成序列张量，被输入到深度神经网络中。</p>
<p>将向量与标记相关联的方法有很多种。这里将介绍两种主要方法：</p>
<ol>
<li><p>对标记做<strong>one-hot 编码</strong>（one-hot encoding）</p>
</li>
<li><p><strong>标记嵌入</strong>［token embedding，通常只用于单词，叫作<strong>词嵌入（word embedding）</strong>］。 </p>
</li>
</ol>
<a id="more"></a>
<p><strong>补充：理解 n-gram 和词袋 </strong></p>
<p>n-gram 是从一个句子中<strong>提取的 N 个（或更少）连续单词的集合</strong>。这一概念中的“单词” 也可以替换为“字符”。 </p>
<p>下面来看一个简单的例子。</p>
<p>考虑句子“The cat sat on the mat.”（“猫坐在垫子上”）。</p>
<p>它可以被分解为以下二元语法（2-grams）的集合。</p>
<p> {“The”, “The cat”, “cat”, “cat sat”, “sat”, “sat on”, “on”, “on the”, “the”, “the mat”, “mat”}</p>
<p> 这个句子也可以被分解为以下三元语法（3-grams）的集合。</p>
<p> {“The”, “The cat”, “cat”, “cat sat”, “The cat sat”, “sat”, “sat on”, “on”, “cat sat on”, “on the”, “the”, “sat on the”, “the mat”, “mat”, “on the mat”} </p>
<p>这样的集合分别叫作二元语法袋（bag-of-2-grams）及三元语法袋（bag-of-3-grams）。这里<strong>袋</strong>（<strong>bag</strong>）这一术语指的是，<strong>我们处理的是标记组成的集合，而不是一个列表或序列，即标记没有特定的顺序</strong>。这一系列分词方法叫作词袋（bag-of-words）。</p>
<p>词袋是一种不保存顺序的分词方法，因此它往往被<strong>用于浅层的语言处理模型</strong>，而不是深度学习模型。提取 n-gram 是一种特征工程，深度学习不需要这种死板而又不稳定的方法，并将其替换为分 层特征学习。深度学习中的一维卷积神经网络和循环神经网络，都能够通过观察连续的单词序列或字符序列来学习单词组和字符组的数据表示，而无须明确知道这些组的存在。因此，这里不进一步讨论 n-gram。<strong>但一定要记住，在使用轻量级的浅层文本处理模型时（比 如 logistic 回归和随机森林），n-gram 是一种功能强大、不可或缺的特征工程工具</strong>。</p>
<h4 id="单词和字符的-one-hot-编码"><a href="#单词和字符的-one-hot-编码" class="headerlink" title="单词和字符的 one-hot 编码"></a>单词和字符的 one-hot 编码</h4><p>one-hot 编码是将标记转换为向量的最常用、最基本的方法。它<strong>将每个单词与一个唯一的整数索引相关联</strong>， 然后将这个整数索引 i 转换为长度为 N 的二进制向量（N 是词表大小），<strong>这个向量只有第 i 个元 素是 1，其余元素都为 0</strong>。 当然，也可以进行字符级的 one-hot 编码。</p>
<!--单词级one-hot编码代码示例-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始数据：每个样本是列表的一个元素（本例中的样本是一个句子，但也可以是一整篇文档） </span></span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据中所有标记的索引</span></span><br><span class="line">token_index = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sample <span class="keyword">in</span> samples:</span><br><span class="line">    <span class="comment"># 利用 split 方法对样本进行分词。在实际应用中，还需要从样本中去掉标点和特殊字符</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sample.split():</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> token_index:</span><br><span class="line">            <span class="comment"># 为每个唯一单词指定一个唯一索引。注意，没有为索引编号 0 指定单词</span></span><br><span class="line">            token_index[word] = len(token_index) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对样本进行分词。只考虑每个样本前 max_length 个单词           </span></span><br><span class="line">max_length = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将结果保存在 results 中</span></span><br><span class="line">results = np.zeros(shape=(len(samples),</span><br><span class="line">                          max_length,</span><br><span class="line">                          max(token_index.values()) + <span class="number">1</span></span><br><span class="line">                          )</span><br><span class="line">                   )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, word <span class="keyword">in</span> list(enumerate(sample.split()))[:max_length]:</span><br><span class="line">        index = token_index.get(word)</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">print(token_index)</span><br><span class="line">&#123;<span class="string">'The'</span>: <span class="number">1</span>, <span class="string">'cat'</span>: <span class="number">2</span>, <span class="string">'sat'</span>: <span class="number">3</span>, <span class="string">'on'</span>: <span class="number">4</span>, <span class="string">'the'</span>: <span class="number">5</span>, <span class="string">'mat.'</span>: <span class="number">6</span>, <span class="string">'dog'</span>: <span class="number">7</span>, <span class="string">'ate'</span>: <span class="number">8</span>, <span class="string">'my'</span>: <span class="number">9</span>, <span class="string">'homework.'</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure>
<!--字符级的 one-hot 编码代码示例-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line"><span class="comment"># 所有可打印的 ASCII 字符</span></span><br><span class="line">characters = string.printable</span><br><span class="line">token_index = dict(zip(characters, range(<span class="number">1</span>, len(characters) + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">max_length = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">results = np.zeros((len(samples), max_length, max(token_index.values()) + <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, character <span class="keyword">in</span> enumerate(sample):</span><br><span class="line">        index = token_index.get(character)</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">print(token_index)</span><br><span class="line">print(results[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'0'</span>: <span class="number">1</span>, <span class="string">'1'</span>: <span class="number">2</span>, <span class="string">'2'</span>: <span class="number">3</span>, <span class="string">'3'</span>: <span class="number">4</span>, <span class="string">'4'</span>: <span class="number">5</span>, <span class="string">'5'</span>: <span class="number">6</span>, <span class="string">'6'</span>: <span class="number">7</span>, <span class="string">'7'</span>: <span class="number">8</span>, <span class="string">'8'</span>: <span class="number">9</span>, <span class="string">'9'</span>: <span class="number">10</span>, <span class="string">'a'</span>: <span class="number">11</span>, <span class="string">'b'</span>: <span class="number">12</span>, <span class="string">'c'</span>: <span class="number">13</span>, <span class="string">'d'</span>: <span class="number">14</span>, <span class="string">'e'</span>: <span class="number">15</span>, <span class="string">'f'</span>: <span class="number">16</span>, <span class="string">'g'</span>: <span class="number">17</span>, <span class="string">'h'</span>: <span class="number">18</span>, <span class="string">'i'</span>: <span class="number">19</span>, <span class="string">'j'</span>: <span class="number">20</span>, <span class="string">'k'</span>: <span class="number">21</span>, <span class="string">'l'</span>: <span class="number">22</span>, <span class="string">'m'</span>: <span class="number">23</span>, <span class="string">'n'</span>: <span class="number">24</span>, <span class="string">'o'</span>: <span class="number">25</span>, <span class="string">'p'</span>: <span class="number">26</span>, <span class="string">'q'</span>: <span class="number">27</span>, <span class="string">'r'</span>: <span class="number">28</span>, <span class="string">'s'</span>: <span class="number">29</span>, <span class="string">'t'</span>: <span class="number">30</span>, <span class="string">'u'</span>: <span class="number">31</span>, <span class="string">'v'</span>: <span class="number">32</span>, <span class="string">'w'</span>: <span class="number">33</span>, <span class="string">'x'</span>: <span class="number">34</span>, <span class="string">'y'</span>: <span class="number">35</span>, <span class="string">'z'</span>: <span class="number">36</span>, <span class="string">'A'</span>: <span class="number">37</span>, <span class="string">'B'</span>: <span class="number">38</span>, <span class="string">'C'</span>: <span class="number">39</span>, <span class="string">'D'</span>: <span class="number">40</span>, <span class="string">'E'</span>: <span class="number">41</span>, <span class="string">'F'</span>: <span class="number">42</span>, <span class="string">'G'</span>: <span class="number">43</span>, <span class="string">'H'</span>: <span class="number">44</span>, <span class="string">'I'</span>: <span class="number">45</span>, <span class="string">'J'</span>: <span class="number">46</span>, <span class="string">'K'</span>: <span class="number">47</span>, <span class="string">'L'</span>: <span class="number">48</span>, <span class="string">'M'</span>: <span class="number">49</span>, <span class="string">'N'</span>: <span class="number">50</span>, <span class="string">'O'</span>: <span class="number">51</span>, <span class="string">'P'</span>: <span class="number">52</span>, <span class="string">'Q'</span>: <span class="number">53</span>, <span class="string">'R'</span>: <span class="number">54</span>, <span class="string">'S'</span>: <span class="number">55</span>, <span class="string">'T'</span>: <span class="number">56</span>, <span class="string">'U'</span>: <span class="number">57</span>, <span class="string">'V'</span>: <span class="number">58</span>, <span class="string">'W'</span>: <span class="number">59</span>, <span class="string">'X'</span>: <span class="number">60</span>, <span class="string">'Y'</span>: <span class="number">61</span>, <span class="string">'Z'</span>: <span class="number">62</span>, <span class="string">'!'</span>: <span class="number">63</span>, <span class="string">'"'</span>: <span class="number">64</span>, <span class="string">'#'</span>: <span class="number">65</span>, <span class="string">'$'</span>: <span class="number">66</span>, <span class="string">'%'</span>: <span class="number">67</span>, <span class="string">'&amp;'</span>: <span class="number">68</span>, <span class="string">"'"</span>: <span class="number">69</span>, <span class="string">'('</span>: <span class="number">70</span>, <span class="string">')'</span>: <span class="number">71</span>, <span class="string">'*'</span>: <span class="number">72</span>, <span class="string">'+'</span>: <span class="number">73</span>, <span class="string">','</span>: <span class="number">74</span>, <span class="string">'-'</span>: <span class="number">75</span>, <span class="string">'.'</span>: <span class="number">76</span>, <span class="string">'/'</span>: <span class="number">77</span>, <span class="string">':'</span>: <span class="number">78</span>, <span class="string">';'</span>: <span class="number">79</span>, <span class="string">'&lt;'</span>: <span class="number">80</span>, <span class="string">'='</span>: <span class="number">81</span>, <span class="string">'&gt;'</span>: <span class="number">82</span>, <span class="string">'?'</span>: <span class="number">83</span>, <span class="string">'@'</span>: <span class="number">84</span>, <span class="string">'['</span>: <span class="number">85</span>, <span class="string">'\\'</span>: <span class="number">86</span>, <span class="string">']'</span>: <span class="number">87</span>, <span class="string">'^'</span>: <span class="number">88</span>, <span class="string">'_'</span>: <span class="number">89</span>, <span class="string">'`'</span>: <span class="number">90</span>, <span class="string">'&#123;'</span>: <span class="number">91</span>, <span class="string">'|'</span>: <span class="number">92</span>, <span class="string">'&#125;'</span>: <span class="number">93</span>, <span class="string">'~'</span>: <span class="number">94</span>, <span class="string">' '</span>: <span class="number">95</span>, <span class="string">'\t'</span>: <span class="number">96</span>, <span class="string">'\n'</span>: <span class="number">97</span>, <span class="string">'\r'</span>: <span class="number">98</span>, <span class="string">'\x0b'</span>: <span class="number">99</span>, <span class="string">'\x0c'</span>: <span class="number">100</span>&#125;</span><br><span class="line">[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br></pre></td></tr></table></figure>
<p>注意，<code>Keras</code> 的内置函数可以对原始文本数据进行单词级或字符级的 one-hot 编码。你应该使用这些函数，因为它们实现了许多重要的特性，比如<strong>从字符串中去除特殊字符、只考虑数据 集中前 N 个最常见的单词</strong>（这是一种常用的限制，以避免处理非常大的输入向量空间）。</p>
<h4 id="使用Keras实现单词级one-hot编码"><a href="#使用Keras实现单词级one-hot编码" class="headerlink" title="使用Keras实现单词级one-hot编码"></a>使用<code>Keras</code>实现单词级one-hot编码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"></span><br><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个分词器（tokenizer），设置为只考虑前 1000 个最常见的单词</span></span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 构建单词索引</span></span><br><span class="line">tokenizer.fit_on_texts(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将字符串转换为整数索引组成的列表</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(samples)</span><br><span class="line"><span class="comment"># 也可以直接得到 one-hot 二进制表示。这个分词器也支持除 one-hot 编码外的其他向量化模式</span></span><br><span class="line">one_hot_results = tokenizer.texts_to_matrix(samples, mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找回单词索引</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(word_index)</span><br><span class="line">&#123;<span class="string">'the'</span>: <span class="number">1</span>, <span class="string">'cat'</span>: <span class="number">2</span>, <span class="string">'sat'</span>: <span class="number">3</span>, <span class="string">'on'</span>: <span class="number">4</span>, <span class="string">'mat'</span>: <span class="number">5</span>, <span class="string">'dog'</span>: <span class="number">6</span>, <span class="string">'ate'</span>: <span class="number">7</span>, <span class="string">'my'</span>: <span class="number">8</span>, <span class="string">'homework'</span>: <span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意：这里的word_index只有9个元素，因为The和the被认为是同一个</strong></p>
<p><strong>补充：<code>Keras</code>分词器<code>Tokenizer</code>的使用介绍</strong></p>
<ul>
<li><p><code>fit_on_texts</code>:将输入的句子列表，生成词典，如上面的samples输入后，就生成了词典，通过<code>tokenizer.word</code>_index查看；</p>
</li>
<li><p><code>texts_to_sequences</code>:将句子转换成单词索引序列，如上面的sequences的值为<code>[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]</code>，代表samples中的句子；</p>
</li>
<li><code>texts_to_matrix</code>:将句子序列转换成token矩阵,也就是one-hot编码</li>
</ul>
<p>不过这里的one_hot_results的shape非三维的，看起来稍微有点奇怪~</p>
<p>one-hot 编码的一种变体是所谓的<strong>one-hot 散列技巧</strong>（one-hot hashing trick），如果词表中唯 一标记的数量太大而无法直接处理，就可以使用这种技巧。这种方法<strong>没有为每个单词显式分配 一个索引并将这些索引保存在一个字典中</strong>，而是将单词散列编码为固定长度的向量，通常用一个非常简单的散列函数来实现。(也就是哈希吧~)</p>
<p>这种方法的主要优点在于，它避免了维护一个显式的单词索引， 从而节省内存并允许数据的在线编码（在读取完所有数据之前，你就可以立刻生成标记向量）。 这种方法有一个缺点，就是可能会出现<strong>散列冲突（hash collision）</strong>，即两个不同的单词可能具有相同的散列值，随后任何机器学习模型观察这些散列值，都无法区分它们所对应的单词。如果散列空间的维度远大于需要散列的唯一标记的个数，散列冲突的可能性会减小。</p>
<!--使用散列技巧的单词级的one-hot编码代码示例-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将单词保存为长度为 1000 的向量。如果单词数量接近 1000 个（或更多），</span></span><br><span class="line"><span class="comment"># 那么会遇到很多散列冲突，这会降低这种编码方法的准确性</span></span><br><span class="line">dimensionality = <span class="number">1000</span></span><br><span class="line">max_length = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">results = np.zeros((len(samples), max_length, dimensionality))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, word <span class="keyword">in</span> list(enumerate(sample.split()))[:max_length]:</span><br><span class="line">        <span class="comment"># 将单词散列为 0~1000 范围内的一个随机整数索引</span></span><br><span class="line">        index = abs(hash(word) % dimensionality)</span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br></pre></td></tr></table></figure>
<h3 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h3><p>将单词与向量相关联还有另一种常用的强大方法，就是使用<strong>密集</strong>的<strong>词向量（word vector）</strong>， 也叫词嵌入（word embedding）。one-hot 编码得到的向量是<strong>二进制的、稀疏的（绝大部分元素都 是 0）、维度很高的</strong>（维度大小等于词表中的单词个数），而词嵌入是<strong>低维的</strong>浮点数向量（即密集向量，与稀疏向量相对）。</p>
<p><img src="/2021/03/15/nlp-vectorize/p1.png" alt></p>
<p>与 one-hot 编码得到的词向量不同，<strong>词嵌入是从数据中学习得到的</strong>。常见的词向量维度是 256、512 或 1024（处理非常大的词表时）。与此相对，one-hot 编码的词向量维度通常为 20 000 或更高（对应包含 20 000 个标记的词表）。因此，词向量可以将更多的信息塞入更低的维度中。</p>
<p>获取词嵌入有两种方法。 </p>
<ul>
<li><strong>在完成主任务（比如文档分类或情感预测）的同时学习词嵌入</strong>。在这种情况下，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的权重相同。</li>
<li>在不同于待解决问题的机器学习任务上<strong>预计算好词嵌入，然后将其加载到模型中</strong>。这些词嵌入叫作<strong>预训练词嵌入（<code>pretrained word embedding</code>）</strong>。</li>
</ul>
<h4 id="利用embedding层学习词嵌入"><a href="#利用embedding层学习词嵌入" class="headerlink" title="利用embedding层学习词嵌入"></a>利用embedding层学习词嵌入</h4><p>要将一个词与一个密集向量相关联，最简单的方法就是随机选择向量。这种方法的问题在于， 得到的嵌入空间没有任何结构。例如，accurate 和 exact 两个词的嵌入可能完全不同，尽管它们 在大多数句子里都是可以互换的。深度神经网络很难对这种杂乱的、非结构化的嵌入空间进行学习。 </p>
<p>说得更抽象一点，<strong>词向量之间的几何关系应该表示这些词之间的语义关系</strong>。词嵌入的作用应该是将人类的语言映射到几何空间中。例如，在一个合理的嵌入空间中，同义词应该被嵌入 到相似的词向量中，一般来说，<strong>任意两个词向量之间的几何距离（比如 <code>L2</code> 距离）应该和这两个 词的语义距离有关（表示不同事物的词被嵌入到相隔很远的点，而相关的词则更加靠近）</strong>。除了距离，你可能还希望嵌入空间中的<strong>特定方向也是有意义的</strong>。</p>
<p>为了更清楚地说明这一点，我们来看一个具体示例。 在图中，四个词被嵌入在二维平面上，这四个词分别是 cat（猫）、dog（狗）、wolf（狼） 和 tiger（虎）。对于我们这里选择的向量表示，这些词之间的某些语义关系可以被编码为几何 变换。例如，从 cat 到 tiger 的向量与从 dog 到 wolf 的向量相等，这个向量可以被解释为“从宠 物到野生动物”向量。同样，从 dog 到 cat 的向量与从 wolf 到 tiger 的向量也相等，它可以被解 释为“从犬科到猫科”向量。</p>
<p><img src="/2021/03/15/nlp-vectorize/p2.png" alt></p>
<p>在真实的词嵌入空间中，常见的有意义的几何变换的例子包括“性别”向量和“复数”向量。 例如，将 king（国王）向量加上 female（女性）向量，得到的是 queen（女王）向量。将 king（国王） 向量加上 plural（复数）向量，得到的是 kings 向量。词嵌入空间通常具有几千个这种可解释的、 并且可能很有用的向量。</p>
<p>一个好的词嵌入空间在很大程度上取决于你的任务，因此，合理的做法是<strong>对每个新任务都学习一个新的嵌入空间</strong>。幸运的是，反向传播让这种 学习变得很简单，而 <code>Keras</code> 使其变得更简单。我们要做的就是学习一个层的权重，这个层就是 Embedding 层。</p>
<p><strong>其实这里的底层原理应该更重要，后面再慢慢研究，这里先看如何应用</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一个embedding层实例化</span></span><br><span class="line">embedding_layer = Embedding(<span class="number">1000</span>, <span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<p><strong>实例化</strong>Embedding 层至少需要两个参数： <strong>标记的个数</strong>（这里是 1000，即最大单词索引 +1）和<strong>嵌入的维度</strong>（这里是 64）</p>
<p>最好将 Embedding 层理解为一个<strong>字典</strong>，将整数索引（表示特定单词）映射为密集向量。它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。</p>
<p>Embedding 层实 际上是一种字典查找：</p>
<p><strong>单词索引 –&gt; Embedding层 –&gt; 对应的词向量</strong></p>
<p>Embedding 层的<strong>输入</strong>是一个<strong>二维</strong>整数张量，其形状为 (samples, sequence_length)， 每个元素是一个整数序列。</p>
<p>它能够嵌入长度可变的序列，例如，对于前一个例子中的 Embedding 层，你可以输入形状为 (32, 10)（32 个长度为 10 的序列组成的批量）或 (64, 15)（64 个长度为 15 的序列组成的批量）的批量。不过一批数据中的<strong>所有序列必须具有相同的长度</strong>（因为需要将它们打包成一个张量），所以<strong>较短的序列应该用 0 填充，较长的序列应该被截断</strong>。 </p>
<p>这个 Embedding 层<strong>返回</strong>一个形状为 (samples, sequence_length, embedding_ dimensionality) 的<strong>三维</strong>浮点数张量。然后可以用<code>RNN</code>层或一维卷积层来处理这个三维张量。</p>
<p> 将一个 Embedding 层实例化时，它的权重（即标记向量的内部字典）最开始是<strong>随机</strong>的， 其他层一样。在训练过程中，利用<strong>反向传播</strong>来逐渐调节这些词向量，改变空间结构以便下游模 型可以利用。一旦训练完成，嵌入空间将会展示大量结构，这种结构专门针对训练模型所要解决的问题。</p>
<p><strong>注意：embedding层实例化、输入和输出的区别</strong></p>
<p>我们将这个想法应用于<code>IMDB</code> 电影评论情感预测任务。首先，我们需要快速准备数据。将电影评论限制为前 10 000 个最常见的单词， 然后将评论长度限制为只有 20 个单词。对于这 10 000 个单词，网络将对每个词都学习一个 8 维嵌入，将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量），然后将这个 张量展平为二维，最后在上面训练一个 Dense 层用于分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, Dense, Embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 作为特征的单词个数</span></span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 在这么多单词后截断文本（这些单词都属于前 max_features 个最常见的单词）</span></span><br><span class="line">max_len = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据加载为整数列表(x是一个第一维为25000，第二维长度不固定的二维数组)</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将整数列表转换成形状为(samples,maxlen) 的二维整数张量(相当于一个截断的操作)</span></span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 指定 Embedding 层的最大输入长度，以便后面将嵌入输入展平。</span></span><br><span class="line"><span class="comment"># Embedding 层激活的形状为 (samples, maxlen, 8)（就是输出的形状）</span></span><br><span class="line">model.add(Embedding(<span class="number">10000</span>, <span class="number">8</span>, input_length=max_len))</span><br><span class="line"><span class="comment"># 将三维的嵌入张量展平成形状为 (samples, maxlen * 8) 的二维张量</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上面添加分类器(这是一个分类任务)</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">    x_train,</span><br><span class="line">    y_train,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    validation_split=<span class="number">0.2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>得到的验证精度约为 76%，考虑到仅查看每条评论的前 20 个单词，这个结果还是相当不错的。但请注意，<strong>仅仅将嵌入序列展开并在上面训练一个 Dense 层，会导致模型对输入序列中的每个单词单独处理，而没有考虑单词之间的关系和句子结构</strong>（举个例子，这个模型可能会将 this movie is a bomb 和 this movie is the bomb 两条都归为负面评论 a）。更好的做法是<strong>在嵌入序列上添加循环层或一维卷积层，将每个序列作为整体来学习特征</strong>。</p>
<h4 id="使用预训练的词嵌入"><a href="#使用预训练的词嵌入" class="headerlink" title="使用预训练的词嵌入"></a>使用预训练的词嵌入</h4><p>有时可用的训练数据很少，以至于只用手头数据无法学习适合特定任务的词嵌入。那么应该怎么办？</p>
<p> 你可以<strong>从预计算的嵌入空间中加载嵌入向量</strong>（你知道这个嵌入空间是高度结构化的，并且具有有用的属性，即抓住了语言结构的一般特点），而不是在解决问题的同时学习词嵌入。</p>
<p>在自然语言处理中使用预训练的词嵌入，其背后的<strong>原理与在图像分类中使用预训练的卷积神经网络是一样的</strong>：没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的， 比如常见的视觉特征或语义特征。</p>
<p>在这种情况下，重复使用在其他问题上学到的特征，这种做法是有道理的。 这种词嵌入通常是利用<strong>词频统计</strong>计算得出的（观察哪些词共同出现在句子或文档中），用到的技术很多，有些涉及神经网络，有些则不涉及。<code>Bengio</code> 等人在 21 世纪初首先研究了一种思路， 就是用无监督的方法计算一个密集的低维词嵌入空间 a，但直到<strong>最有名且最成功</strong>的词嵌入方案之一<code>word2vec</code>算法发布之后，这一思路才开始在研究领域和工业应用中取得成功。</p>
<p><code>word2vec</code>算法 由 Google 的 <code>Tomas Mikolov</code>于 2013 年开发，其维度抓住了特定的语义属性，比如性别。 有许多预计算的词嵌入数据库，你都可以下载并在<code>Keras</code>的 Embedding 层中使用。 <code>word2vec</code> 就是其中之一。另一个常用的是 <code>GloVe</code>（global vectors for word representation，<strong>词表示全局向量</strong>），由斯坦福大学的研究人员于 2014 年开发。这种嵌入方法基于对词共现统计矩阵进行因式分解。其开发者已经公开了数百万个英文标记的预计算嵌入，它们都是从维基百科数据 和 Common Crawl 数据得到的。 </p>
<p>我们来看一下如何在<code>Keras</code>模型中使用<code>GloVe</code>嵌入。同样的方法也适用于<code>word2ve</code>c 嵌入或 其他词嵌入数据库。这个例子还可以改进前面刚刚介绍过的文本分词技术，即<strong>从原始文本开始， 一步步进行处理</strong>。</p>
<!--首先下载训练数据并处理-->：<br><br>下载链接：<a href="http://mng.bz/0tIo，下载原始" target="_blank" rel="noopener">http://mng.bz/0tIo，下载原始</a> <code>IMDB</code> 数据集并解压<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imdb_dir = <span class="string">'D:/python_project/深度学习/keras_t/samples/aclImdb'</span></span><br><span class="line">train_dir = os.path.join(imdb_dir, <span class="string">'train'</span>)</span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">'neg'</span>, <span class="string">'pos'</span>]:</span><br><span class="line">    dir_name = os.path.join(train_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(dir_name):</span><br><span class="line">        <span class="keyword">if</span> fname[<span class="number">-4</span>:] == <span class="string">'.txt'</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">'neg'</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<!--对数据进行分词-->
<p>利用本节前面介绍过的概念，我们对文本进行分词，并将其划分为训练集和验证集。因为预训练的词嵌入对训练数据很少的问题特别有用（否则，针对于具体任务的嵌入可能效果更好）， 所以我们又添加了以下限制：将训练数据限定为前 200 个样本。因此，你需要在读取 200 个样本之后学习对电影评论进行分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 100 个单词后截断评论</span></span><br><span class="line">max_len = <span class="number">100</span></span><br><span class="line"><span class="comment"># 在 200 个样本上训练</span></span><br><span class="line">training_samples = <span class="number">200</span></span><br><span class="line"><span class="comment"># 在 10 000 个样本上验证</span></span><br><span class="line">validation_samples = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 只考虑数据集中前 10 000 个最常见的单词</span></span><br><span class="line">max_words = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words=max_words)</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line"></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line">data = pad_sequences(sequences, maxlen=max_len)</span><br><span class="line"></span><br><span class="line">labels = np.asarray(labels)</span><br><span class="line">print(<span class="string">'Shape of data tensor:'</span>, data.shape)</span><br><span class="line">print(<span class="string">'Shape of label tensor:'</span>, labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据划分为训练集和验证集，但首先要打乱数据</span></span><br><span class="line"><span class="comment"># 因为一开始数据中的样本是排好序的（所有负面评论都在前面，然后是所有正面评论）</span></span><br><span class="line">indices = np.arange(data.shape[<span class="number">0</span>])</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line">data = data[indices]</span><br><span class="line">labels = labels[indices]</span><br><span class="line"></span><br><span class="line">x_train = data[:training_samples]</span><br><span class="line">y_train = labels[:training_samples]</span><br><span class="line">x_val = data[training_samples: training_samples + validation_samples]</span><br><span class="line">y_val = labels[training_samples: training_samples + validation_samples]</span><br></pre></td></tr></table></figure>
<p><strong>补充：pad_sequences()有什么用</strong></p>
<p><code>pad_sequences(sequences, maxlen=None)</code></p>
<p><code>maxlen</code>设置最大的序列长度，长于该长度的序列将会截短，短于该长度的序列将会填充</p>
<!--下载`GloVe`词嵌入-->
<p>打开 <a href="https://nlp.stanford.edu/projects/glove，下载" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove，下载</a> 2014 年英文维基百科的预计算嵌入。这是一个 822 MB 的压缩文件，文件名是 <code>glove.6B.zip</code>，里面包含 400 000 个单词（或非单词的标记） 的 100 维嵌入向量。(官网下载不了可以找百度云链接)，解压文件。</p>
<!--对嵌入进行预处理-->
<p>对解压后的文件（一个<code>.txt</code>文件）进行解析，构建一个将单词(字符串)映射为其向量表示的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">glove_dir = <span class="string">'D:/python_project/深度学习/keras_t/samples/glove.6B'</span></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line">f = open(os.path.join(glove_dir, <span class="string">'glove.6B.100d.txt'</span>), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.split()</span><br><span class="line">    word = values[<span class="number">0</span>]</span><br><span class="line">    coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">    embeddings_index[word] = coefs</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p>接下来，需要构建一个可以加载到 Embedding 层中的嵌入矩阵。它必须是一个形状为 (max_words, embedding_dim) 的矩阵，对于单词索引（在分词时构建）中索引为 i 的单词， <strong>这个矩阵的元素 i 就是这个单词对应的 embedding_dim 维向量</strong>。注意，索引 0 不应该代表任何单词或标记，它只是一个占位符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">embeddings_matrix = np.zeros((max_words, embedding_dim))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    <span class="keyword">if</span> i &lt; max_words:</span><br><span class="line">        embedding_vector = embeddings_index.get(word)</span><br><span class="line">        <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 嵌入索引（embeddings_index）中找不到的词，其嵌入向量全为 0</span></span><br><span class="line">            embeddings_matrix[i] = embedding_vector</span><br></pre></td></tr></table></figure>
<!--定义模型-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(max_words, embedding_dim, input_length=max_len))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<!--在模型中加入Glove嵌入-->
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.layers[0].set_weights([embeddings_matrix])</span><br><span class="line">model.layers[0].trainable = False</span><br></pre></td></tr></table></figure>
<!--训练模型与评估模型-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>, validation_data=(x_val, y_val))</span><br><span class="line">model.save_weights(<span class="string">'pre_trained_glove_mode.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制结果</span></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line">loss = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(acc)+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training_acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation_acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>模型很快就开始过拟合，考虑到训练样本很少，这一点也不奇怪。出于同样的原因，验证精度的波动很大，但似乎达到了接近 60%。</p>
<p><em>因为这里的图像波动比较较大就不展示了</em></p>
<p>注意，你的结果可能会有所不同。训练样本数太少，所以模型性能严重依赖于你选择的 200 个样本，而样本是随机选择的。如果你得到的结果很差，可以尝试重新选择 200 个不同的 随机样本，你可以将其作为练习（在现实生活中无法选择自己的训练数据）。 </p>
<p>你也可以在不加载预训练词嵌入、也不冻结嵌入层的情况下训练相同的模型。在这种情况下， 你将会学到针对任务的输入标记的嵌入。<strong>如果有大量的可用数据，这种方法通常比预训练词嵌入更加强大</strong>，但本例只有 200 个训练样本。</p>
<p>我们来试一下这种方法：其实只要把下面这两行代码注释掉就可以了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在模型中加入Glove嵌入</span></span><br><span class="line"><span class="comment"># model.layers[0].set_weights([embeddings_matrix])</span></span><br><span class="line"><span class="comment"># model.layers[0].trainable = False</span></span><br></pre></td></tr></table></figure>
<p>验证精度停留在 50% 多一点。因此，在本例中，预训练词嵌入的性能要优于与任务一起学 习的嵌入。如果增加样本数量，情况将很快发生变化，你可以把它作为一个练习。</p>
<!--最后，我们在测试数据上评估模型-->
<p>首先，你需要对测试数据进行分词，方法和流程和前面训练集的分词是一致的，所以其实这里可以封装成一个方法，分别调用生成训练集和测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">test_dir = os.path.join(imdb_dir, <span class="string">'test'</span>)</span><br><span class="line">labels = []</span><br><span class="line">texts = []</span><br><span class="line"><span class="keyword">for</span> label_type <span class="keyword">in</span> [<span class="string">'neg'</span>, <span class="string">'pos'</span>]:</span><br><span class="line">    dir_name = os.path.join(test_dir, label_type)</span><br><span class="line">    <span class="keyword">for</span> fname <span class="keyword">in</span> sorted(os.listdir(dir_name)):</span><br><span class="line">        <span class="keyword">if</span> fname[<span class="number">-4</span>:] == <span class="string">'.txt'</span>:</span><br><span class="line">            f = open(os.path.join(dir_name, fname), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">            texts.append(f.read())</span><br><span class="line">            f.close()</span><br><span class="line">            <span class="keyword">if</span> label_type == <span class="string">'neg'</span>:</span><br><span class="line">                labels.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels.append(<span class="number">1</span>)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">x_test = pad_sequences(sequences, maxlen=max_len)</span><br><span class="line">y_test = np.asarray(labels)</span><br></pre></td></tr></table></figure>
<!--在测试集上评估模型-->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.load_weights(<span class="string">'pre_trained_glove_mode.h5'</span>)</span><br><span class="line">print(model.evaluate(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p>测试精度达到了令人震惊的 56% ！只用了很少的训练样本，得到这样的结果很不容易。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这里讲了以下内容：</p>
<ul>
<li>将原始文本转换为神经网络能够处理的格式。 </li>
<li>使用 <code>Keras</code> 模型的 Embedding 层来学习针对特定任务的标记嵌入。 </li>
<li>使用预训练词嵌入在小型自然语言处理问题上获得额外的性能提升。</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/one-hot/" rel="tag"># one-hot</a>
          
            <a href="/tags/embedding/" rel="tag"># embedding</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/03/12/dl-computer-vision3/" rel="next" title="深度学习用于计算机视觉(下篇)">
                <i class="fa fa-chevron-left"></i> 深度学习用于计算机视觉(下篇)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/04/23/nlp-rnn/" rel="prev" title="理解循环神经网络">
                理解循环神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="chenxi">
            
              <p class="site-author-name" itemprop="name">chenxi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">103</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">197</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://docle.github.io/" title="Docle" target="_blank">Docle</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://transformerswsz.github.io/" title="Swift" target="_blank">Swift</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习用于文本和序列"><span class="nav-text">深度学习用于文本和序列</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文本向量化"><span class="nav-text">文本向量化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#单词和字符的-one-hot-编码"><span class="nav-text">单词和字符的 one-hot 编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用Keras实现单词级one-hot编码"><span class="nav-text">使用Keras实现单词级one-hot编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用词嵌入"><span class="nav-text">使用词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#利用embedding层学习词嵌入"><span class="nav-text">利用embedding层学习词嵌入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用预训练的词嵌入"><span class="nav-text">使用预训练的词嵌入</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
