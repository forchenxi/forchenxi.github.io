<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="deep learning,nlp,BERT,NER,transformer,simple lexicon,">










<meta name="description" content="Simple Lexicon代码剖析Simple Lexicon的原理比较简单，至少不涉及模型架构方面的改写，只是将所有词的embedding融合到字符的embedding中（详情请看原论文，上一篇），其项目源码其实是用了Lattice LSTM的源代码，然后在此基础上改成了python3.6的版本，并且加上词向量融合部分的代码。 项目地址：GitHub - v-mipeng/LexiconAug">
<meta name="keywords" content="deep learning,nlp,BERT,NER,transformer,simple lexicon">
<meta property="og:type" content="article">
<meta property="og:title" content="Simple Lexicon代码剖析">
<meta property="og:url" content="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/index.html">
<meta property="og:site_name" content="Sunrise">
<meta property="og:description" content="Simple Lexicon代码剖析Simple Lexicon的原理比较简单，至少不涉及模型架构方面的改写，只是将所有词的embedding融合到字符的embedding中（详情请看原论文，上一篇），其项目源码其实是用了Lattice LSTM的源代码，然后在此基础上改成了python3.6的版本，并且加上词向量融合部分的代码。 项目地址：GitHub - v-mipeng/LexiconAug">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/mean_pooling.png">
<meta property="og:image" content="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/frequency.png">
<meta property="og:updated_time" content="2024-09-21T10:48:27.265Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Simple Lexicon代码剖析">
<meta name="twitter:description" content="Simple Lexicon代码剖析Simple Lexicon的原理比较简单，至少不涉及模型架构方面的改写，只是将所有词的embedding融合到字符的embedding中（详情请看原论文，上一篇），其项目源码其实是用了Lattice LSTM的源代码，然后在此基础上改成了python3.6的版本，并且加上词向量融合部分的代码。 项目地址：GitHub - v-mipeng/LexiconAug">
<meta name="twitter:image" content="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/mean_pooling.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/">





  <title>Simple Lexicon代码剖析 | Sunrise</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sunrise</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">`长路漫漫，唯剑作伴`</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/04/27/nlp-simple-lexicon-code/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sunrise">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Simple Lexicon代码剖析</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-04-27T15:55:04+08:00">
                2024-04-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/自然语言处理/" itemprop="url" rel="index">
                    <span itemprop="name">自然语言处理</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Simple-Lexicon代码剖析"><a href="#Simple-Lexicon代码剖析" class="headerlink" title="Simple Lexicon代码剖析"></a>Simple Lexicon代码剖析</h2><p>Simple Lexicon的原理比较简单，至少不涉及模型架构方面的改写，只是将所有词的embedding融合到字符的embedding中（详情请看原论文，上一篇），其项目源码其实是用了Lattice LSTM的源代码，然后在此基础上改成了python3.6的版本，并且加上词向量融合部分的代码。</p>
<p>项目地址：<a href="https://github.com/v-mipeng/LexiconAugmentedNER" target="_blank" rel="noopener">GitHub - v-mipeng/LexiconAugmentedNER: Reject complicated operations for incorporating lexicon for Chinese NER.</a></p>
<p>下面直接进行代码分析，在剖析代码的过程中联系论文来深入理解</p>
<h3 id="data-initialization"><a href="#data-initialization" class="headerlink" title="data_initialization"></a>data_initialization</h3><p>首先在构建语料库这里，依然还是有</p>
<p>build_alphabet、build_gaz_file，这两个方法与Lattice LSTM是一致的；</p>
<p>有变化的是build_gaz_alphabet，新增了一个count参数（用于统计词频）</p>
<p>在Lattice LSTM中build_gaz_alphabet方法是建立gaz_alphabet，即找到了<strong>所有可能的词语</strong>（具体逻辑就是利用trie属性保存的词典，然后调用gaz.enumerateMatchList，这里面调用的又是trie.enumerateMatch方法）</p>
<p>在Simple Lexicon中这个方法内部也是先找到所有可能的词语（<strong>但是也会返回单个的字</strong>），其次是计算词频；</p>
<a id="more"></a>
<p>在Lattice LSTM中,enumerateMatch是这么实现的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enumerateMatch</span><span class="params">(self, word, space=<span class="string">"_"</span>, backward=False)</span>:</span></span><br><span class="line">    matched = []</span><br><span class="line">    <span class="comment"># while len(word) &gt; 1 does not keep character itself, while word keed character itself</span></span><br><span class="line">    <span class="keyword">while</span> len(word) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> self.search(word):  <span class="comment"># 判断当前的word中，以首个字到最后一个字之间是否可以全部连成一个词</span></span><br><span class="line">            matched.append(space.join(word[:]))  <span class="comment"># 如果可以连成词，则添加到结果集中</span></span><br><span class="line">        <span class="keyword">del</span> word[<span class="number">-1</span>]  <span class="comment"># 如果连不成词，那就减去最后一个字再重试</span></span><br><span class="line">    <span class="keyword">return</span> matched  <span class="comment"># 返回的是word list中，以0号元素为开头的所有可能的词（注意是连续的）</span></span><br></pre></td></tr></table></figure>
<p>在Simple Lexicon中，enumerateMatch是这么实现的（改动点其实就是while循环的条件，从而达到单个字也可以返回的目的）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enumerateMatch</span><span class="params">(self, word, space=<span class="string">"_"</span>, backward=False)</span>:</span>  <span class="comment"># space=‘’</span></span><br><span class="line">       matched = []</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span> len(word) &gt; <span class="number">0</span>:</span><br><span class="line">           <span class="keyword">if</span> self.search(word):</span><br><span class="line">               matched.append(space.join(word[:]))</span><br><span class="line">           <span class="keyword">del</span> word[<span class="number">-1</span>]</span><br><span class="line">       <span class="keyword">return</span> matched</span><br></pre></td></tr></table></figure>
<p>Simple Lexicon之所以要返回单个字就是为了也统计字的频率，当count参数为True时</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> count:</span><br><span class="line">    entitys.sort(key=<span class="keyword">lambda</span> x: -len(x))</span><br><span class="line">    <span class="keyword">while</span> entitys:</span><br><span class="line">        longest = entitys[<span class="number">0</span>]</span><br><span class="line">        longest_index = self.gaz_alphabet.get_index(longest)</span><br><span class="line">        self.gaz_count[longest_index] = self.gaz_count.get(longest_index, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        gazlen = len(longest)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gazlen):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, gazlen+<span class="number">1</span>):</span><br><span class="line">                covering_gaz = longest[i:j]</span><br><span class="line">                <span class="keyword">if</span> covering_gaz <span class="keyword">in</span> entitys:</span><br><span class="line">                    entitys.remove(covering_gaz)</span><br></pre></td></tr></table></figure>
<p>entitys是一个句子匹配到的所有可能的词（包含这个句子的所有单个字符），统计词频时是先对entitys按照元素长度<strong>从长到短排序</strong>，将长的元素计数之后，<strong>然后再去entitys里面找剩下的有没有这个词的子集（包含本身），将其从entitys中删掉</strong>。这里为什么要这么做呢？</p>
<p>原论文中说：“请注意，如果w被与词典匹配的另一个子序列覆盖，则w的频率不会增加。这防止了较短单词的频率总是小于覆盖它的较长单词的频率的问题”。</p>
<p>我感觉作者应该是说反了，应该是防止较短单词的频率总是<strong>大于</strong>覆盖它的较长单词的频率的问题</p>
<p>这样统计好的词频就保存在了self.gaz_count中，其key是这个词在gaz_alphabet中的索引，value是出现的次数（在当前数据集如训练集、测试集等出现的频率）</p>
<h3 id="generate-instance-with-gaz"><a href="#generate-instance-with-gaz" class="headerlink" title="generate_instance_with_gaz"></a>generate_instance_with_gaz</h3><p>read_instance_with_gaz：在Lattice LSTM中最终生成两个列表instance_texts和instance_ids，其分别又包含5个列表</p>
<p>instance_texts.append([words, biwords, chars, gazs, labels])<br>instance_ids.append([word_Ids, biword_Ids, char_Ids, gaz_Ids, label_Ids])</p>
<p>在Simple Lexicon中</p>
<p>instance_texts.append([words, biwords, chars, gazs, labels])<br>instance_Ids.append([word_Ids, biword_Ids, char_Ids, gaz_Ids, label_Ids, gazs, gazs_count, gaz_char_Id, layergazmasks, gazchar_masks, bert_text_ids])</p>
<p>即instance_ids中包含11个列表，多了6个列表，下面我们来分别剖析每个列表的含义，但是需要借助例子帮助理解，例如句子：[‘历’, ‘任’, ‘公’, ‘司’, ‘副’, ‘总’, ‘经’, ‘理’, ‘、’, ‘总’, ‘工’, ‘程’, ‘师’, ‘，’]</p>
<p><strong>gazs</strong>：这里的gazs与Lattice LSTM不一样，这里的gazs虽然列表长度与句子长度一致，但是每个元素又是一个固定长度为4的列表，而且4个元素也分别为列表，分别表示句子当前位置的字符在所有的分词结果中属于B、M、E、S的情况（与论文相对应）。</p>
<p>gazs初始时（length=14）：[[[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []], [[], [], [], []]]</p>
<p>第1个字“历”匹配的分词结果有两种：“历”和“历任”，所以针对“历”是单个字（Single），gazs的第一个元素应该是</p>
<p>[[], [], [], [1]]</p>
<p>针对“历任”(历处在Begin位置)，所以gazs的第一个元素应该是：[[1], [], [], [1]]，相应的对于“任”(End)字，即句子的第二个位置，应该是：[[], [], [1], []]</p>
<p>然后要注意的是，这里实际存放到gazs中并不是1和0，而是”历”、”历任”在gaz_alphabet中的索引，如”历任”的索引是219，”历”的索引是47，接下来还需要针对四个列表中空的列表补个0，即[[219], [0], [0], [47]]，[[0], [0], [219], [0]]</p>
<p><em>虽然gazs的形式变了，但是gaz_Ids的格式与Lattice LSTM一致</em></p>
<p>到这还没结束，后面还有padding操作，还是这里的例子，gazs的第一个元素为：</p>
<p>[[219], [0], [0], [47]]</p>
<p>max_gazlist记录了当前句子的某个位置B、M、E、S集合中元素最多的数量（假设为3）</p>
<p>这里就需要将gazs中每个元素中的四个列表长度统一成3，即得到：</p>
<p>[[219, 0, 0], [0, 0, 0], [0, 0, 0], [47, 0, 0]]</p>
<p>在mask的过程中，gazmask记录了padding的索引，即1是padding的，0是真实的</p>
<p>[[0, 1, 1], [0, 1, 1], [0, 1, 1], [0, 1, 1]]，然后将gazmask添加到layergazmasks中</p>
<p><strong>gazs_count</strong>: 其shape与gazs完全一致，相应位置保存的是这些词的词频，如第一个元素为[[305], [], [], [1]]，305表示”历任”出现的次数，1表示”历”出现的次数，第二个元素为[[], [], [305], []]，305表示”历任”出现的次数。</p>
<p>同样的在gazs进行补0操作之后，也要在gazs_count中相应位置补1（表示1次）</p>
<p>接下来与gazs中的padding保持一致，给padding的元素记录的count=0，这里gazs_count第1个元素本来是：</p>
<p>[[305], [1], [1], [1]]</p>
<p>进行与之对应的padding操作后变成：</p>
<p>[[305, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]</p>
<p><strong>gaz_char_Id</strong>: 其shape与gazs完全一致，相应位置保存的是这些词拆成单个字符之后，该字符在word_alphabet中的索引，例如”历任”可以拆成”历”和”任”在word_alphabet中的索引分别为28，58，那么第一个元素为</p>
<p>[[[28, 58]],[[0]],[[0]],[[28]]]，即相当于使用[28,58]代替219，[28]代替315，因为历任是一个整体，而且可能还会有其他以”历”字开头的词，所以这里的char_id又多了一层列表，即[28, 58]；之后同样的针对空列表补零([0])</p>
<p>接下来也是padding操作，max_gazcharlen记录了当前句子最长的候选词的长度（假设为4）</p>
<p>所以这里需要将gaz_char_Id的每个元素中的char_ids列表长度统一成4（padding操作），即得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [[<span class="number">28</span>, <span class="number">58</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], </span><br><span class="line">    [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], </span><br><span class="line">    [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], </span><br><span class="line">    [[<span class="number">28</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>注意这里不仅将每个char_ids长度统一成了4，而且将每个元素中四个列表的长度统一为了3（与gazs和gazs_count一致）</p>
<p>这个例子中，gazs和gazs_count的size为：(14, 4, 3)</p>
<p>而gaz_char_Id的size为：(14, 4, 3, 4)，最后多了一维</p>
<p>同理，在padding的时候，gazcharmask也保存了padding的索引，例如这里第1个元素padding之后得到的gazcharmask为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], </span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], </span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], </span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>最后将gazcharmask添加到gazchar_masks中</p>
<p><strong>layergazmasks</strong>: 其size与gazs和gazs_count一致，记录的是gazs_mask</p>
<p><strong>gazchar_masks</strong>: 其size与gaz_char_Id一致，记录的是gazcharmask</p>
<p><strong>bert_text_ids</strong>：利用bert tokenizer获取本条句子的id，在开始添加[CLS]，在结尾添加[SEP]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>, do_lower_case=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">texts = [<span class="string">'[CLS]'</span>] + words + [<span class="string">'[SEP]'</span>]</span><br><span class="line">bert_text_ids = tokenizer.convert_tokens_to_ids(texts)</span><br></pre></td></tr></table></figure>
<p>这个例子中words = [‘历’, ‘任’, ‘公’, ‘司’, ‘副’, ‘总’, ‘经’, ‘理’, ‘、’, ‘总’, ‘工’, ‘程’, ‘师’, ‘，’]</p>
<p><em>有可能其实并没有使用bert_text_ids？</em></p>
<h3 id="batchify-with-label"><a href="#batchify-with-label" class="headerlink" title="batchify_with_label"></a>batchify_with_label</h3><p>传入的参数是上面得到的instance_Ids，其最外围size为训练集样本数量，然后每一个元素则为一个size=11的列表，即包含了上面解析的11个元素；</p>
<p>在batchify_with_label内部，将一批batch_size大小的数据统一成相同的size的张量返回</p>
<p>分别使用以下变量名替换原来的11个元素</p>
<p>word_ids –&gt; word_seq_tensor</p>
<p>biword_Ids –&gt; biword_seq_tensor</p>
<p>char_Ids，这个丢弃了没有使用</p>
<p>gaz_Ids  –&gt; gazs 这个没做处理，返回的gazs是取得gaz_Ids(后续其实也并没有使用，这代码写的也是很离谱…)</p>
<p>label_Ids  –&gt; label_seq_tensor</p>
<p>gazs –&gt; layer_gaz_tensor</p>
<p>gazs_count –&gt;  gaz_count_tensor</p>
<p>gaz_char_Id –&gt; gaz_chars_tensor</p>
<p>layergazmasks –&gt; gaz_mask_tensor</p>
<p>gazchar_masks –&gt; gazchar_mask_tensor</p>
<p>bert_text_ids  –&gt; bert_seq_tensor</p>
<p>另外还返回了 word_seq_lengths（同一批次内每个样本长度），mask，bert_mask</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">word_seq_lengths = torch.LongTensor(list(map(len, words)))</span><br><span class="line">max_seq_len = word_seq_lengths.max()</span><br><span class="line">word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len))).long()</span><br><span class="line">......</span><br><span class="line">mask = autograd.Variable(torch.zeros((batch_size, max_seq_len))).byte()</span><br><span class="line">bert_mask = autograd.Variable(torch.zeros((batch_size, max_seq_len+<span class="number">2</span>))).long()</span><br></pre></td></tr></table></figure>
<p>mask和word_seq_tensor是一样的，即一开始统一成了(batch_size, max_seq_length)，之后针对每个样本(for b, seqlen in enumerate(word_seq_lengths)，补上真实的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask[b, :seqlen] = torch.Tensor([<span class="number">1</span>]*int(seqlen))</span><br><span class="line">bert_mask[b, :seqlen+<span class="number">2</span>] = torch.LongTensor([<span class="number">1</span>]*int(seqlen+<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>而layer_gaz_tensor、gaz_count_tensor、gaz_mask_tensor则一开始初始化为(batch_size, max_seq_length, 4, max_gaz_num)大小，然后针对每个样本再补上真实值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gaz_num = [len(layer_gazs[i][<span class="number">0</span>][<span class="number">0</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size)]</span><br><span class="line">max_gaz_num = max(gaz_num)</span><br><span class="line">layer_gaz_tensor = torch.zeros(batch_size, max_seq_len, <span class="number">4</span>, max_gaz_num).long()</span><br><span class="line">gaz_count_tensor = torch.zeros(batch_size, max_seq_len, <span class="number">4</span>, max_gaz_num).float()</span><br><span class="line">gaz_mask_tensor = torch.ones(batch_size, max_seq_len, <span class="number">4</span>, max_gaz_num).byte()</span><br></pre></td></tr></table></figure>
<p>gaz_chars_tensor、gazchar_mask_tensor一开始初始化为(batch_size, max_seq_length, 4, max_gaz_num, max_gaz_len)大小，但是注意后者的初始值为1（即1是padding的索引，0是实际的，与gaz_mask_tensor一致）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gaz_len = [len(gaz_chars[i][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size)]</span><br><span class="line">max_gaz_len = max(gaz_len)</span><br><span class="line">gaz_chars_tensor = torch.zeros(batch_size, max_seq_len, <span class="number">4</span>, max_gaz_num, max_gaz_len).long()</span><br><span class="line">gazchar_mask_tensor = torch.ones(batch_size, max_seq_len, <span class="number">4</span>, max_gaz_num, max_gaz_len).byte()</span><br></pre></td></tr></table></figure>
<h3 id="SeqModel"><a href="#SeqModel" class="headerlink" title="SeqModel"></a>SeqModel</h3><p>包含以下层或模块</p>
<p>两个embedding层(加载预训练的词向量)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.gaz_embedding = nn.Embedding(data.gaz_alphabet.size(), self.gaz_emb_dim)</span><br><span class="line">self.word_embedding = nn.Embedding(data.word_alphabet.size(), self.word_emb_dim)</span><br></pre></td></tr></table></figure>
<p>NERmodel(支持lstm，cnn和transformer三种结构)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.model_type == <span class="string">'lstm'</span>:</span><br><span class="line">    lstm_hidden = self.hidden_dim</span><br><span class="line">	<span class="keyword">if</span> self.bilstm_flag:</span><br><span class="line">    	self.hidden_dim *= <span class="number">2</span></span><br><span class="line">	self.NERmodel = NERmodel(model_type=<span class="string">'lstm'</span>, input_dim=char_feature_dim, hidden_dim=lstm_hidden, num_layer=self.lstm_layer, biflag=self.bilstm_flag)</span><br></pre></td></tr></table></figure>
<p>dropout层、线性层和CRF模型（自行实现的CRF模型）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.drop = nn.Dropout(p=data.HP_dropout)</span><br><span class="line">self.hidden2tag = nn.Linear(self.hidden_dim, data.label_alphabet_size+<span class="number">2</span>)</span><br><span class="line">self.crf = CRF(data.label_alphabet_size, self.gpu)</span><br></pre></td></tr></table></figure>
<h4 id="NERmodel"><a href="#NERmodel" class="headerlink" title="NERmodel"></a>NERmodel</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NERmodel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_type, input_dim, hidden_dim, num_layer, dropout=<span class="number">0.5</span>, gpu=True, biflag=True)</span>:</span></span><br><span class="line">        super(NERmodel, self).__init__()</span><br><span class="line">        self.model_type = model_type</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'lstm'</span>:</span><br><span class="line">            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layer, batch_first=<span class="literal">True</span>, bidirectional=biflag)</span><br><span class="line">            self.drop = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'cnn'</span>:</span><br><span class="line">            self.cnn = CNNmodel(input_dim, hidden_dim, num_layer, dropout, gpu)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention model</span></span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'transformer'</span>:</span><br><span class="line">            self.attention_model = AttentionModel(d_input=input_dim, d_model=hidden_dim, d_ff=<span class="number">2</span>*hidden_dim, head=<span class="number">4</span>, num_layer=num_layer, dropout=dropout)</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> self.attention_model.parameters():</span><br><span class="line">                <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                    nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, mask=None)</span>:</span></span><br><span class="line">        feature_out_d = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'lstm'</span>:</span><br><span class="line">            hidden = <span class="literal">None</span></span><br><span class="line">            feature_out, hidden = self.lstm(input, hidden)</span><br><span class="line"></span><br><span class="line">            feature_out_d = self.drop(feature_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'cnn'</span>:</span><br><span class="line">            feature_out_d = self.cnn(input)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_type == <span class="string">'transformer'</span>:</span><br><span class="line">            feature_out_d = self.attention_model(input, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feature_out_d</span><br></pre></td></tr></table></figure>
<p>NERmodel方面的实现也比较简单，lstm网络就是直接使用的pytorch内置的nn.LSTM。</p>
<h4 id="模型前向计算"><a href="#模型前向计算" class="headerlink" title="模型前向计算"></a>模型前向计算</h4><p>NERmodel中有三个主要方法：get_tags、neg_log_likelihood_loss、forward</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood_loss</span><span class="params">(self, gaz_list, word_inputs, biword_inputs, word_seq_lengths, layer_gaz, gaz_count, gaz_chars, gaz_mask, gazchar_mask, mask, batch_label, batch_bert, bert_mask)</span>:</span></span><br><span class="line"></span><br><span class="line">    tags, _ = self.get_tags(gaz_list, word_inputs, biword_inputs, layer_gaz, gaz_count,gaz_chars, gaz_mask, gazchar_mask, mask, word_seq_lengths, batch_bert, bert_mask)</span><br><span class="line"></span><br><span class="line">    total_loss = self.crf.neg_log_likelihood_loss(tags, mask, batch_label)</span><br><span class="line">    <span class="comment"># scores, tag_seq = self.crf._viterbi_decode(tags, mask)</span></span><br><span class="line">    score, tag_seq = self.crf(tags, mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss, tag_seq</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, gaz_list, word_inputs, biword_inputs, word_seq_lengths,layer_gaz, gaz_count,gaz_chars, gaz_mask,gazchar_mask, mask, batch_bert, bert_mask)</span>:</span></span><br><span class="line"></span><br><span class="line">    tags, gaz_match = self.get_tags(gaz_list, word_inputs, biword_inputs, layer_gaz, gaz_count,gaz_chars, gaz_mask, gazchar_mask, mask, word_seq_lengths, batch_bert, bert_mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scores, tag_seq = self.crf._viterbi_decode(tags, mask)</span></span><br><span class="line">    score, tag_seq = self.crf(tags, mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tag_seq, gaz_match</span><br></pre></td></tr></table></figure>
<p>在模型训练时，其实调用的是neg_log_likelihood_loss，因为需要计算损失，而模型在预测时调用的是forward；不过这两个方法内部都调用了get_tags，这个才是比较核心的方法。</p>
<p>下面主要介绍方法内部比较核心的部分</p>
<p>首先是将tensor传入embedding层和dropout层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word_inputs shape(batch_size, seq_length) </span></span><br><span class="line"><span class="comment"># word_embs shape(batch_size, seq_length, embedding_dim)</span></span><br><span class="line">word_embs = self.word_embedding(word_inputs)</span><br><span class="line"><span class="keyword">if</span> self.model_type != <span class="string">'transformer'</span>:</span><br><span class="line">    word_inputs_d = self.drop(word_embs)   <span class="comment"># (b,l,we)</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    word_inputs_d = word_embs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的layer_gaz就是generate_instance_with_gaz中的gazs，</span></span><br><span class="line"><span class="comment"># shape(batch_size, seq_length, 4, max_gaz_num)</span></span><br><span class="line"><span class="comment"># gaz_embeds shape(batch_size, seq_length, 4, maz_gaz_num, gaz_embedding_dim)</span></span><br><span class="line">gaz_embeds = self.gaz_embedding(layer_gaz)</span><br><span class="line"><span class="keyword">if</span> self.model_type != <span class="string">'transformer'</span>:</span><br><span class="line">    gaz_embeds_d = self.drop(gaz_embeds)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    gaz_embeds_d = gaz_embeds</span><br></pre></td></tr></table></figure>
<p>其次是针对gaz_embeds_d中padding的部分置0，根据gaz_mask可以知道哪些位置是padding的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gaz_mask_input的shape是(batch_size, seq_length, 4, max_gaz_num)</span></span><br><span class="line"><span class="comment"># 这里先在最后增加1维，然后前面4个维度都是只复制1次，最后1维复制gaz_emb_dim次</span></span><br><span class="line"><span class="comment"># 所以gaz_mask变成了(batch_size, seq_length, 4, max_gaz_num, gaz_embedding_dim)</span></span><br><span class="line">gaz_mask = gaz_mask_input.unsqueeze(<span class="number">-1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, self.gaz_emb_dim)</span><br><span class="line"><span class="comment"># 举个例子，gaz_mask_input的第3、4维的元素是</span></span><br><span class="line">[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化成gaz_mask后，第4、5维的元素是</span></span><br><span class="line">[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="comment"># 0</span></span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="comment"># 0</span></span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="comment"># 0</span></span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line">[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],  <span class="comment"># 0</span></span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],  <span class="comment"># 1</span></span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],  <span class="comment"># 1</span></span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,  ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将gaz_mask中为1的位置(表示是填充的位置)，在gaz_embeds_d中置0</span></span><br><span class="line">gaz_embeds = gaz_embeds_d.data.masked_fill_(gaz_mask.data, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>接下来要把gaz_embeds和words_input_d融合到一起，但是这两个张量的size不一样，首先需要将gaz_embeds转化成words_input_d一样的size，在转化之前原论文中考虑如何使用这些集合中的这些词向量的问题，在原论文中提出两种方式：</p>
<p>第1种，直接每个集合（这里的集合指的是B、M、E、S集合）内部计算平均值</p>
<p><img src="/2024/04/27/nlp-simple-lexicon-code/mean_pooling.png" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gaz_num = (gaz_mask_input == <span class="number">0</span>).sum(dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>).float()  <span class="comment"># (b,l,4,1)</span></span><br><span class="line">gaz_embeds = gaz_embeds.sum(<span class="number">-2</span>) / gaz_num  <span class="comment"># (b,l,4,ge)/(b,l,4,1)</span></span><br></pre></td></tr></table></figure>
<p>论文中表示这种方式效果不理想，所以提出一个权重算法用于利用词信息，使用每个单词的频率作为其权重的指示，不过<strong>注意这里的词频是指某一集合中的词在统计数据集中的次数除以4个集合中所有词的次数，根据计算得到的结果再乘以4来决定这个词的embedding占用多少权重</strong></p>
<p><img src="/2024/04/27/nlp-simple-lexicon-code/frequency.png" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">count_sum = torch.sum(gaz_count, dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (b,l,4,gn)</span></span><br><span class="line">count_sum = torch.sum(count_sum, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (b,l,1,1)</span></span><br><span class="line"></span><br><span class="line">weights = gaz_count.div(count_sum)  <span class="comment"># (b,l,4,g)</span></span><br><span class="line">weights = weights*<span class="number">4</span></span><br><span class="line">weights = weights.unsqueeze(<span class="number">-1</span>)</span><br><span class="line">gaz_embeds = weights*gaz_embeds  <span class="comment"># (b,l,4,g,e)</span></span><br><span class="line">gaz_embeds = torch.sum(gaz_embeds, dim=<span class="number">3</span>)  <span class="comment"># (b,l,4,e)</span></span><br></pre></td></tr></table></figure>
<p>再然后只需要把gaz_embeds四个集合的tensor拼接起来，然后再拼接到word_input_d上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gaz_embeds_cat = gaz_embeds.view(batch_size, seq_len, <span class="number">-1</span>)  <span class="comment"># ( b,l,4*ge)</span></span><br><span class="line"><span class="comment"># word_input_cat shape(batch_size, seq_length, word_embedding_dim+4xgaz_embedding_dim)</span></span><br><span class="line">word_input_cat = torch.cat([word_inputs_d, gaz_embeds_cat], dim=<span class="number">-1</span>)  <span class="comment"># (b,l,we+4*ge)</span></span><br></pre></td></tr></table></figure>
<p>最后就是传入lstm网络和最后一层线性分类曾，也就是lstm的输入张量size为(batch_size, seq_length, word_embedding_dim+4xgaz_embedding_dim)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature_out_d shape(batch_size, seq_length, hidden_dimx2)</span></span><br><span class="line">feature_out_d = self.NERmodel(word_input_cat)</span><br><span class="line"><span class="comment"># tags shape(batch_size, seq_length, label_alphabet_size+2)</span></span><br><span class="line">tags = self.hidden2tag(feature_out_d)</span><br></pre></td></tr></table></figure>
<p>上述内容是第一阶段对代码的剖析，当时仅想使用lstm结构的NERmodel，所以对transformer结构的NERmodel以及联合Bert的方法没有深入研究（而且当时我以为transformer的结构和BERT的预训练权重一定要联合使用，这是误解，后面会解释）。</p>
<h3 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h3><h4 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h4><p>在/data目录新建一个文件夹，存放你的数据集，数据集的格式与Lattice LSTM的数据集格式一模一样，BIO/BIOES均可，但是每行一个字符。</p>
<p>有几个预训练的词向量权重需要下载一下，下载地址也是与Lattice LSTM一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char_emb = <span class="string">"./data/gigaword_chn.all.a2b.uni.ite50.vec"</span></span><br><span class="line">bichar_emb = <span class="string">"./data/gigaword_chn.all.a2b.bi.ite50.vec"</span></span><br><span class="line">gaz_file = <span class="string">"./data/ctb.50d.vec"</span></span><br></pre></td></tr></table></figure>
<p>以上预训练词向量的下载地址为：<a href="https://pan.baidu.com/s/1pLO6T9D" target="_blank" rel="noopener">https://pan.baidu.com/s/1pLO6T9D</a></p>
<p>char_emb是单个字符的预训练词向量，大小为5.2MB<br>bichar_emb是二元字符的预训练词向量，大小为1.8G，这个是命名实体识别发展中的一种过渡方法，其实后续大家基本都不怎么使用了，效果提升有限，所以可以不使用，将bichar_emd置空即可<br>gaz_file是预训练的词语向量，大小为325.7MB</p>
<h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><p>优化器：Adamax<br>学习率lr: 0.0015（不断衰减）<br>batch_size: 1<br>hidden_dim：300<br>num_layer: 4（这个参数只有当model_type=’transfomer’时有效）<br>lstm_layers: 1（这个参数只有当model_type=’lstm’时有效）<br>model_type：默认lstm（可以改为cnn、transformer）<br>use_biword: False<br>use_count: True，这个就是simple lexicon的核心思想参数，使用词频来设置BMES集合中的词的权重<br>use_bert: False</p>
<p>按照Simple Lexicon源代码的默认配置，仅使用一层的lstm网络（双向），输入的词向量维度为50+4x50，搭配hidden_dim=300，实际测试训练效果确实比Lattice LSTM好很多，而且因为参数量很小，速度也很快。</p>
<p>如果数据量比较多，想要批量化训练，也是可以的，可以增大batch_size的值，但是经过我实际测试需要修改一行代码，也就是crf.py中的这部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(len(back_points) - <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">    pointer = torch.gather(back_points[idx], <span class="number">1</span>, pointer.contiguous().view(batch_size, <span class="number">1</span>))</span><br><span class="line">    pointer = pointer.squeeze(<span class="number">1</span>)  <span class="comment"># batch_size &gt; 1时，新增的逻辑</span></span><br><span class="line">    decode_idx[idx] = pointer.data</span><br></pre></td></tr></table></figure></p>
<p>但是Simple Lexicon有个缺点，当训练的数据量很大时，保存的”.dset”文件也会很大，会比较占内存。</p>
<h4 id="结合BERT"><a href="#结合BERT" class="headerlink" title="结合BERT"></a>结合BERT</h4><p>一开始我以为simple lexicon结合BERT是指使用BERT的网络结构以及BERT的预训练权重，所以我同时将<br>model_type设置为transformer，use_bert设置为True。</p>
<p>关于use_bert=True部分的代码逻辑是这样的<br>模型定义部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.use_bert:</span><br><span class="line">    self.bert_encoder = BertModel.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> self.bert_encoder.parameters():</span><br><span class="line">        p.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<p>直接使用transformer包里封装好的方法导入BERT，且使用的是预训练的中文BERT，这一步首次运行会先下载模型权重和配置。<br>前向计算部分<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat bert feature</span></span><br><span class="line"><span class="keyword">if</span> self.use_bert:</span><br><span class="line">    seg_id = torch.zeros(bert_mask.size()).long().cuda()</span><br><span class="line">    outputs = self.bert_encoder(batch_bert, bert_mask, seg_id)</span><br><span class="line">    outputs = outputs[<span class="number">0</span>][:, <span class="number">1</span>:<span class="number">-1</span>, :]</span><br><span class="line">    word_input_cat = torch.cat([word_input_cat, outputs], dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">feature_out_d = self.NERmodel(word_input_cat)</span><br></pre></td></tr></table></figure></p>
<p>在前向计算时，直接将输入的句子输入到bert_encoder中，得到的输出是768维的向量(outputs)，而word_input_cat在上面介绍了是一个50+4x50=250维的待输入到NERmodel的向量，也就是说如果use_bert=True，这里是先把输入句子完整走了一遍Bert模型，再将得到的输出与原来的输入拼接在一起组成新的输入，最终再输入到NERmodel，而NERmodel无论是transformer还是lstm都是可以的。</p>
<p>我们详细看一下transformer类型的NERmodel的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.model_type == <span class="string">'transformer'</span>:</span><br><span class="line">    self.NERmodel = NERmodel(model_type=<span class="string">'transformer'</span>, input_dim=char_feature_dim, hidden_dim=self.hidden_dim, num_layer=self.num_layer, dropout=data.HP_dropout)</span><br></pre></td></tr></table></figure>
<p>NERmodel<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># attention model</span></span><br><span class="line"><span class="keyword">if</span> self.model_type == <span class="string">'transformer'</span>:</span><br><span class="line">    self.attention_model = AttentionModel(d_input=input_dim, d_model=hidden_dim, d_ff=<span class="number">2</span>*hidden_dim, head=<span class="number">4</span>, num_layer=num_layer, dropout=dropout)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> self.attention_model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_input, d_model, d_ff, head, num_layer, dropout)</span>:</span></span><br><span class="line">        super(AttentionModel, self).__init__()</span><br><span class="line">        c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">        attn = MultiHeadedAttention(head, d_model, dropout)   <span class="comment"># 这里第2个参数本该是d_input</span></span><br><span class="line">        ff = PositionwiseFeedForward(d_model, d_ff, dropout)  <span class="comment"># 这里第1个参数本该是d_input</span></span><br><span class="line">        layer = EncoderLayer(d_model, c(attn), c(ff), dropout)</span><br><span class="line">        self.layers = clones(layer, num_layer)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        self.posi = PositionalEncoding(d_model, dropout)  <span class="comment"># 这里第1个参数本该是d_input</span></span><br><span class="line">        self.input2model = nn.Linear(d_input, d_model)   <span class="comment"># 这里先过一个线性层，将1018--&gt;转为300</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="comment"># x: embedding (b,l,we)</span></span><br><span class="line">        x = self.posi(self.input2model(x))</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>首先这个transformer结构的NERmodel的层数为4（num_layer），注意力头的个数为4，d_ff的大小为600（2*hidden_dim），这明显与Base BERT的架构不一致，所以可以证实use_bert和transformer结构的NERmodel并非强关联；<br>其次这里d_input是1018（768+250）,d_model=hidden_dim=300，正常的transformer模型只有d_input和d_ff，但是这里在forward中，先将输入过了一个线性层，即将输入的维度大小从d_input转为d_model，然后再输入到transformer模型中，不难想象这一操作可能会损失较多信息，实践也证明，use_bert+transformer模型的效果并不理想，远不如lstm。<br>当然不使用这一线性层也是不行的，因为输入的维度1018太过尴尬，很难设置注意力头的个数（要保证input_dim可以整除head_nums），但是可以考虑将hidden_dim设置为1000，即通过这一线性层将1018转为1000，丢失的信息就比较少了，但是实际测试效果依然不理想。按照我的经验，使用transformer的效果一直不如lstm，除了使用与bert一致的结构。</p>
<p>既然这样，可以使用use_bert+lstm的配置，因为输入是1018维，我设置了对照组，hidden_dim=300的默认值以及hidden_dim=1000的对比值，实验结果表明hidden_dim=1000的效果明显更好。<br>另外当hidden_dim=300，use_bert=True+lstm比use_bert=False+lstm略好一丢丢，并不明显。</p>
<p>另外use_bert=True时，最终得到的模型权重文件是比较大的，等于base_bert的权重大小+NERmodel的大小，但是虽然参数量比较大，实际上base_bert这部分参数在训练时是不会改变的（Non-trainable params），只有NERmodel部分才是会改变的参数(trainable params)。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/BERT/" rel="tag"># BERT</a>
          
            <a href="/tags/NER/" rel="tag"># NER</a>
          
            <a href="/tags/transformer/" rel="tag"># transformer</a>
          
            <a href="/tags/simple-lexicon/" rel="tag"># simple lexicon</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2024/04/13/nlp-simple-lexicon-paper/" rel="next" title="Simplify the Usage of Lexicon in Chinese NER论文">
                <i class="fa fa-chevron-left"></i> Simplify the Usage of Lexicon in Chinese NER论文
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2024/06/01/spider-anhui/" rel="prev" title="反爬（十）">
                反爬（十） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="chenxi">
            
              <p class="site-author-name" itemprop="name">chenxi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">116</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">213</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://docle.github.io/" title="Docle" target="_blank">Docle</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://transformerswsz.github.io/" title="Swift" target="_blank">Swift</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-Lexicon代码剖析"><span class="nav-text">Simple Lexicon代码剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-initialization"><span class="nav-text">data_initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generate-instance-with-gaz"><span class="nav-text">generate_instance_with_gaz</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batchify-with-label"><span class="nav-text">batchify_with_label</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SeqModel"><span class="nav-text">SeqModel</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NERmodel"><span class="nav-text">NERmodel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型前向计算"><span class="nav-text">模型前向计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#项目实战"><span class="nav-text">项目实战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前期准备"><span class="nav-text">前期准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超参数"><span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结合BERT"><span class="nav-text">结合BERT</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
