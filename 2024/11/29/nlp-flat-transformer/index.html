<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="deep learning,nlp,NER,flat lattice transformer,fastNLP,">










<meta name="description" content="flat lattice transformer论文解读论文《FLAT: Chinese NER Using Flat-Lattice Transformer》 LeeSureman/Flat-Lattice-Transformer: code for ACL 2020 paper: FLAT: Chinese NER Using Flat-Lattice Transformer (github.">
<meta name="keywords" content="deep learning,nlp,NER,flat lattice transformer,fastNLP">
<meta property="og:type" content="article">
<meta property="og:title" content="中文命名实体识别——flat lattice transformer">
<meta property="og:url" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/index.html">
<meta property="og:site_name" content="Sunrise">
<meta property="og:description" content="flat lattice transformer论文解读论文《FLAT: Chinese NER Using Flat-Lattice Transformer》 LeeSureman/Flat-Lattice-Transformer: code for ACL 2020 paper: FLAT: Chinese NER Using Flat-Lattice Transformer (github.">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p1.png">
<meta property="og:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p2.jpg">
<meta property="og:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p3.jpg">
<meta property="og:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p4.png">
<meta property="og:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p5.png">
<meta property="og:updated_time" content="2025-02-18T14:53:50.744Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="中文命名实体识别——flat lattice transformer">
<meta name="twitter:description" content="flat lattice transformer论文解读论文《FLAT: Chinese NER Using Flat-Lattice Transformer》 LeeSureman/Flat-Lattice-Transformer: code for ACL 2020 paper: FLAT: Chinese NER Using Flat-Lattice Transformer (github.">
<meta name="twitter:image" content="http://yoursite.com/2024/11/29/nlp-flat-transformer/p1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2024/11/29/nlp-flat-transformer/">





  <title>中文命名实体识别——flat lattice transformer | Sunrise</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sunrise</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">`长路漫漫，唯剑作伴`</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/11/29/nlp-flat-transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sunrise">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">中文命名实体识别——flat lattice transformer</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-11-29T10:31:54+08:00">
                2024-11-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/自然语言处理/" itemprop="url" rel="index">
                    <span itemprop="name">自然语言处理</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="flat-lattice-transformer"><a href="#flat-lattice-transformer" class="headerlink" title="flat lattice transformer"></a>flat lattice transformer</h2><h3 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h3><p>论文《FLAT: Chinese NER Using Flat-Lattice Transformer》</p>
<p><a href="https://github.com/LeeSureman/Flat-Lattice-Transformer" target="_blank" rel="noopener">LeeSureman/Flat-Lattice-Transformer: code for ACL 2020 paper: FLAT: Chinese NER Using Flat-Lattice Transformer (github.com)</a></p>
<p>解读略</p>
<h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><h4 id="Windows环境"><a href="#Windows环境" class="headerlink" title="Windows环境"></a>Windows环境</h4><p>python和pytorch版本可以根据自己的环境情况，其他包按照官网的说明安装<br>numpy包，安装1.16.4版本需要C++环境，安装不上<br>安装超过1.24的版本会因为fitlog库中使用了np.str而报错，虽然1.20之后就废弃了np.str之后，但是安装&lt;1.24的版本还是能跑的<br>我这里安装了<strong>1.23.5</strong></p>
<p>pytorch，项目要求安装版本为1.2.0，但是因为我的cuda版本是12.0，安装不上1.2.0的版本，一开始安装的是2.0.1的版本，但是因为版本太新，在<code>torch.optim.lr_scheduler</code>中其<code>LambdaLR</code>（以及其他类型的调度器）均继承的是LRScheduler类，<br>而在fastNLP中，其要求为：</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> isinstance(lr_scheduler, torch.optim.lr_scheduler._LRScheduler):</span><br><span class="line">    self.scheduler = lr_scheduler</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">f"Expect torch.optim.lr_scheduler for LRScheduler. Got <span class="subst">&#123;type(lr_scheduler)&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<p>所以就会导致报错，我将pytorch的版本降低至1.7.1.解决了此问题<br>在此版本中，所有的调度器继承的是：<code>_LRScheduler</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LambdaLR</span><span class="params">(_LRScheduler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, optimizer: Optimizer, lr_lambda: Union[Callable[[int], float], List[Callable[[int], float]]], last_epoch: int=...)</span> -&gt; <span class="keyword">None</span>:</span> ...</span><br></pre></td></tr></table></figure></p>
<h4 id="服务器环境（centos或Linux）"><a href="#服务器环境（centos或Linux）" class="headerlink" title="服务器环境（centos或Linux）"></a>服务器环境（centos或Linux）</h4><p>python3.8的版本，安装不了<code>FastNLP==0.5.0</code><br>python3.6.9的版本，所有的包可以安装和flat lattice transformer项目完全一致的版本<br>PyTorch: 1.2.0<br>FastNLP: 0.5.0<br>Numpy: 1.16.4<br>fitlog: 0.9.13<br>pytz: 2024.2<br>但是11G的显存还是没法跑我的数据集（batch_size=10），上4090D吧</p>
<p>conda 可以直接创建需要的python版本的虚拟环境，而无需预先安装指定的python版本<br>使用conda先创建了python3.7.3的环境，实践证明安装不上FastNLP==0.5.0<br>之后将python改为3.6.9，可以安装（并且注意FastNLP只能用pip安装）<br>fitlog也只能用pip安装</p>
<p>cuda 12.4版本，安装pytorch==1.2.0（cuda=10.0），运行不了代码：<br><code>RuntimeError: cublas runtime error : the GPU program failed to execute at /tmp/pip-req-build-p5q91txh/aten/src/THC/THCBlas.cu:331</code><br>这个错误大概就是pytorch安装的cuda版本和现在cuda版本不匹配（差距太大）导致的<br>果然重新安装pytorch=1.7.1的版本，cuda=11.0就可以运行了</p>
<h3 id="代码剖析"><a href="#代码剖析" class="headerlink" title="代码剖析"></a>代码剖析</h3><p>项目代码：<a href="https://github.com/LeeSureman/Flat-Lattice-Transformer" target="_blank" rel="noopener">https://github.com/LeeSureman/Flat-Lattice-Transformer</a><br>flat参考了transformer-XL中的相对位置编码：<br><a href="https://zhuanlan.zhihu.com/p/271984518" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/271984518</a></p>
<p>flat瘦身日记：<a href="https://zhuanlan.zhihu.com/p/509248057" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/509248057</a></p>
<p>发现simple lexicon和flat都是忽略空白字符的，空白字符按说肯定是有意义的：<br>venv/Lib/site-packages/fastNLP/io/file_reader.py，改动125行以及下面几行</p>
<p>原来的代码是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> line_idx, line <span class="keyword">in</span> enumerate(f, <span class="number">1</span>):</span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="keyword">if</span> line == <span class="string">''</span>:</span><br><span class="line">        <span class="keyword">if</span> len(sample):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                res = parse_conll(sample)</span><br><span class="line">                sample = []</span><br><span class="line">                <span class="keyword">yield</span> line_idx, res</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">if</span> dropna:</span><br><span class="line">                    logger.warning(<span class="string">'Invalid instance which ends at line: &#123;&#125; has been dropped.'</span>.format(line_idx))</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Invalid instance which ends at line: &#123;&#125;'</span>.format(line_idx))</span><br><span class="line">    <span class="keyword">elif</span> line.startswith(<span class="string">'#'</span>):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sample.append(line.split())</span><br></pre></td></tr></table></figure></p>
<p>改为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> line_idx, line <span class="keyword">in</span> enumerate(f, <span class="number">1</span>):</span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="keyword">if</span> line == <span class="string">''</span>:</span><br><span class="line">        <span class="keyword">if</span> len(sample):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                res = parse_conll(sample)</span><br><span class="line">                sample = []</span><br><span class="line">                <span class="keyword">yield</span> line_idx, res</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">if</span> dropna:</span><br><span class="line">                    logger.warning(<span class="string">'Invalid instance which ends at line: &#123;&#125; has been dropped.'</span>.format(line_idx))</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Invalid instance which ends at line: &#123;&#125;'</span>.format(line_idx))</span><br><span class="line">    <span class="keyword">elif</span> line.startswith(<span class="string">'#'</span>):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pairs = line.split()  <span class="comment"># 有改动</span></span><br><span class="line">        <span class="keyword">if</span> len(pairs) == <span class="number">1</span>:</span><br><span class="line">            word = <span class="string">' '</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word = pairs[<span class="number">0</span>]</span><br><span class="line">        label = pairs[<span class="number">-1</span>]</span><br><span class="line">        sample.append([word, label])</span><br></pre></td></tr></table></figure></p>
<h4 id="数据处理部分"><a href="#数据处理部分" class="headerlink" title="数据处理部分"></a>数据处理部分</h4><p><strong>load_xxx_ner()方法</strong><br>内部主要使用了<code>from fastNLP.io.loader import ConllLoader</code>来加载数据集，应该是在内部封装了一些操作，其实不太用关注实现细节。<br>这个方法返回三个结果，并且将结果保存到缓存文件，如果下载运行时缓存文件存在，就直接加载缓存文件就好了，无需重复执行这一方法。</p>
<ul>
<li><p>datasets：字典类型，包含三个key（train、test和dev）,每个key对应的value都是DataSet类型（fastNLP中定义的），DataSet中有个fields_array字段，其也为字典类型，对应一系列key、value。<br>包括chars：size为(样本数量, 每个句子的字符数量)<br>target：size为(样本数量, 每个句子的字符数量)<br>bigrams：size为(样本数量, 每个句子中的所有二元词的数量)，其实二元词的数量与句子的字符数量一致，二元词就是每个位置的字符+下一个位置的字符组合，如果是最后一个字符，那就加上\<end>\<br>seq_len：size为(样本数量, 1)，其中存储的时chars的长度</end></p>
</li>
<li><p>vocab：字典类型，包含三个key(char、label和bigram)，每个key对应的value都是Vocabulary类型（fastNLP中定义的），char是训练集、测试集和验证集中所有的字符，label是所有的标签（包括一个\&lt;pad>和一个\&lt;unk>），bigram是三个数据集中所有的二元词。</p>
</li>
<li>embedding：字典类型，包含两个key(char、bigram)，每个key对应的value都是StaticEmbedding类型(fastNLP中定义的)，embedding[‘char’]是根据vocab中的char从预训练的单个字向量文件(gigaword_chn.all.a2b.uni.ite50.vec)中读取的后续会用到的字预训练向量；embedding[‘bigram’]是根据vocab中的bigram从预训练的二元词向量文(gigaword_chn.all.a2b.bi.ite50.vec)件中读取的后续会用到的二元词预训练向量。</li>
</ul>
<p><strong>equip_chinese_ner_with_lexicon()方法</strong><br>针对上一方法中得到的datasets、vocab和embedding进一步处理，首先根据传入的w_list参数构建w_trie，w_list中保存了从ctb.50d.vec文件中读取的所有长度大于等于2的词，w_trie以Trie的数据结构保存（详见Lattice LSTM中的代码），然后利用Trie数据结构方便获取字的可选词的特性，获取datasets中所有char的可选词<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_skip_path</span><span class="params">(chars, w_trie)</span>:</span></span><br><span class="line">    sentence = <span class="string">''</span>.join(chars)</span><br><span class="line">    result = w_trie.get_lexicon(sentence)</span><br><span class="line">    <span class="comment"># print(result)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> datasets.items():</span><br><span class="line">    <span class="comment"># 这里使用了 functools.partial 来固定 get_skip_path 函数的 w_trie 参数。这个处理函数将被应用于 chars 字段，并将结果存储在 lexicons 字段中。</span></span><br><span class="line">    v.apply_field(partial(get_skip_path, w_trie=w_trie), <span class="string">'chars'</span>, <span class="string">'lexicons'</span>)</span><br><span class="line">    v.apply_field(copy.copy, <span class="string">'chars'</span>, <span class="string">'raw_chars'</span>)</span><br><span class="line">    v.add_seq_len(<span class="string">'lexicons'</span>, <span class="string">'lex_num'</span>)</span><br><span class="line">    <span class="comment">#start index</span></span><br><span class="line">    v.apply_field(<span class="keyword">lambda</span> x: list(map(<span class="keyword">lambda</span> y: y[<span class="number">0</span>], x)), <span class="string">'lexicons'</span>, <span class="string">'lex_s'</span>)  </span><br><span class="line">    <span class="comment"># end index</span></span><br><span class="line">    v.apply_field(<span class="keyword">lambda</span> x: list(map(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], x)), <span class="string">'lexicons'</span>, <span class="string">'lex_e'</span>)</span><br></pre></td></tr></table></figure></p>
<p>关于第一步增加lexicons字段的步骤举个例子，假设输入的某一条样本chars为：<br><code>[&#39;被&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;原&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;噢&#39;, &#39;斯&#39;, &#39;曼&#39;, &#39;·&#39;, &#39;阿&#39;, &#39;木&#39;, &#39;克&#39;, &#39;;&#39;, &#39;原&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;被&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;喀&#39;, &#39;什&#39;, &#39;纳&#39;, &#39;迪&#39;, &#39;尔&#39;, &#39;农&#39;, &#39;业&#39;, &#39;科&#39;, &#39;技&#39;, &#39;有&#39;, &#39;限&#39;, &#39;公&#39;, &#39;司&#39;, &#39;;&#39;]</code><br>得到的result为：<br><code>[[0, 1, &#39;被告&#39;], [2, 3, &#39;(反&#39;], [3, 4, &#39;反诉&#39;], [5, 6, &#39;原告&#39;], [6, 7, &#39;告)&#39;], [7, 8, &#39;):&#39;], [13, 14, &#39;阿木&#39;], [15, 16, &#39;克;&#39;], [16, 17, &#39;;原&#39;], [17, 18, &#39;原告&#39;], [19, 20, &#39;(反&#39;], [20, 21, &#39;反诉&#39;], [22, 23, &#39;被告&#39;], [23, 24, &#39;告)&#39;], [24, 25, &#39;):&#39;], [26, 27, &#39;喀什&#39;], [28, 29, &#39;纳迪&#39;], [28, 30, &#39;纳迪尔&#39;], [29, 30, &#39;迪尔&#39;], [31, 32, &#39;农业&#39;], [31, 33, &#39;农业科&#39;], [33, 34, &#39;科技&#39;], [34, 35, &#39;技有&#39;], [35, 36, &#39;有限&#39;], [37, 38, &#39;公司&#39;], [38, 39, &#39;司;&#39;]]</code><br>关于raw_chars和lex_num字段从代码中很简单就能看得懂，重点是’lex_s’和’lex_e’，因为这两个字段是以lexicons字段为基础，而lexicons的例子就是上面的result，y[0]就是这个词的开始索引，y[1]是这个词的结束索引（索引是指这个字在原始句子的位置索引）。这里就跟原论文的扁平lattice思想挂钩了<br><img src="/2024/11/29/nlp-flat-transformer/p1.png" alt><br>例如这里”重庆”的”重”开始索引为1，结束索引为2，”人和药店”的”人”开始索引为3，结束索引为6，当然论文中的这张图索引下标是从1开始的，而实际程序中下标从0开始。</p>
<p>原论文中还提到，会将句子中所有匹配的可选词，都放到句子的最后（就像上图中一样），代码就是下面这部分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat</span><span class="params">(ins)</span>:</span></span><br><span class="line">    chars = ins[<span class="string">'chars'</span>]</span><br><span class="line">    lexicons = ins[<span class="string">'lexicons'</span>]</span><br><span class="line">    result = chars + list(map(<span class="keyword">lambda</span> x: x[<span class="number">2</span>], lexicons))  <span class="comment"># 就是把lexicon中的词拼接到char列表的最后</span></span><br><span class="line">    <span class="comment"># print('lexicons:&#123;&#125;'.format(lexicons))</span></span><br><span class="line">    <span class="comment"># print('lex_only:&#123;&#125;'.format(list(filter(lambda x:x[2],lexicons))))</span></span><br><span class="line">    <span class="comment"># print('result:&#123;&#125;'.format(result))</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pos_s</span><span class="params">(ins)</span>:</span></span><br><span class="line">    lex_s = ins[<span class="string">'lex_s'</span>]  <span class="comment"># 这里进来每次只会针对一条数据执行，实际上apply方法里有个循环，会针对每条数据都执行</span></span><br><span class="line">    seq_len = ins[<span class="string">'seq_len'</span>]</span><br><span class="line">    pos_s = list(range(seq_len)) + lex_s   <span class="comment"># 词的索引放在字的最后面，这是开始索引（对应图中的第1行）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pos_s</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pos_e</span><span class="params">(ins)</span>:</span></span><br><span class="line">    lex_e = ins[<span class="string">'lex_e'</span>]</span><br><span class="line">    seq_len = ins[<span class="string">'seq_len'</span>]</span><br><span class="line">    pos_e = list(range(seq_len)) + lex_e  <span class="comment"># 词的索引放在字的最后面，这是结束索引（对应图中的第2行）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> datasets.items():</span><br><span class="line">    v.apply(concat, new_field_name=<span class="string">'lattice'</span>)   <span class="comment"># 调用concat方法，输入结果存放到新字段lattice</span></span><br><span class="line">    v.set_input(<span class="string">'lattice'</span>)   <span class="comment"># 作为模型的输入，目前为止，v中的10个字段，只有lattice是输入(is_input=True)</span></span><br><span class="line">    v.apply(get_pos_s, new_field_name=<span class="string">'pos_s'</span>)</span><br><span class="line">    v.apply(get_pos_e, new_field_name=<span class="string">'pos_e'</span>)</span><br><span class="line">    v.set_input(<span class="string">'pos_s'</span>, <span class="string">'pos_e'</span>)  <span class="comment"># 开始索引和结束索引也作为输入</span></span><br></pre></td></tr></table></figure></p>
<p>这里继续使用一个例子来解释lattice字段的生成过程(concat方法)：<br>chars：<code>[&#39;被&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;原&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;噢&#39;, &#39;斯&#39;, &#39;曼&#39;, &#39;·&#39;, &#39;阿&#39;, &#39;木&#39;, &#39;克&#39;, &#39;;&#39;, &#39;原&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;被&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;喀&#39;, &#39;什&#39;, &#39;纳&#39;, &#39;迪&#39;, &#39;尔&#39;, &#39;农&#39;, &#39;业&#39;, &#39;科&#39;, &#39;技&#39;, &#39;有&#39;, &#39;限&#39;, &#39;公&#39;, &#39;司&#39;, &#39;;&#39;]</code><br>lexicons：<code>[[0, 1, &#39;被告&#39;], [2, 3, &#39;(反&#39;], [3, 4, &#39;反诉&#39;], [5, 6, &#39;原告&#39;], [6, 7, &#39;告)&#39;], [7, 8, &#39;):&#39;], [13, 14, &#39;阿木&#39;], [15, 16, &#39;克;&#39;], [16, 17, &#39;;原&#39;], [17, 18, &#39;原告&#39;], [19, 20, &#39;(反&#39;], [20, 21, &#39;反诉&#39;], [22, 23, &#39;被告&#39;], [23, 24, &#39;告)&#39;], [24, 25, &#39;):&#39;], [26, 27, &#39;喀什&#39;], [28, 29, &#39;纳迪&#39;], [28, 30, &#39;纳迪尔&#39;], [29, 30, &#39;迪尔&#39;], [31, 32, &#39;农业&#39;], [31, 33, &#39;农业科&#39;], [33, 34, &#39;科技&#39;], [34, 35, &#39;技有&#39;], [35, 36, &#39;有限&#39;], [37, 38, &#39;公司&#39;], [38, 39, &#39;司;&#39;]]</code><br>得到result：<code>[&#39;被&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;原&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;噢&#39;, &#39;斯&#39;, &#39;曼&#39;, &#39;·&#39;, &#39;阿&#39;, &#39;木&#39;, &#39;克&#39;, &#39;;&#39;, &#39;原&#39;, &#39;告&#39;, &#39;(&#39;, &#39;反&#39;, &#39;诉&#39;, &#39;被&#39;, &#39;告&#39;, &#39;)&#39;, &#39;:&#39;, &#39;喀&#39;, &#39;什&#39;, &#39;纳&#39;, &#39;迪&#39;, &#39;尔&#39;, &#39;农&#39;, &#39;业&#39;, &#39;科&#39;, &#39;技&#39;, &#39;有&#39;, &#39;限&#39;, &#39;公&#39;, &#39;司&#39;, &#39;;&#39;, &#39;被告&#39;, &#39;(反&#39;, &#39;反诉&#39;, &#39;原告&#39;, &#39;告)&#39;, &#39;):&#39;, &#39;阿木&#39;, &#39;克;&#39;, &#39;;原&#39;, &#39;原告&#39;, &#39;(反&#39;, &#39;反诉&#39;, &#39;被告&#39;, &#39;告)&#39;, &#39;):&#39;, &#39;喀什&#39;, &#39;纳迪&#39;, &#39;纳迪尔&#39;, &#39;迪尔&#39;, &#39;农业&#39;, &#39;农业科&#39;, &#39;科技&#39;, &#39;技有&#39;, &#39;有限&#39;, &#39;公司&#39;, &#39;司;&#39;]</code></p>
<p>pos_s和pos_e分别是开始索引和结束索引，对应图中的第1行和第2行<br>pos_s: <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 0, 2, 3, 5, 6, 7, 13, 15, 16, 17, 19, 20, 22, 23, 24, 26, 28, 28, 29, 31, 31, 33, 34, 35, 37, 38]</code><br>pos_e: <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 1, 3, 4, 6, 7, 8, 14, 16, 17, 18, 20, 21, 23, 24, 25, 27, 29, 30, 30, 32, 33, 34, 35, 36, 38, 39]</code></p>
<p>vocab增加两个key分别为”word”和”lattice”，”word”是w_list中的所有词，”lattice”是datasets三个数据集中所有的chars+lexicons。<br>embedding也增加两个key分别为”word”和”lattice”，分别对应vocab[“word”]的预训练词向量(ctb.50d.vec)和vocab[“lattice”]的预训练词向量(yangjie_word_char_mix.txt，这个是融合了ctb.50d.vec+gigaword_chn.all.a2b.uni.ite50.vec)。<br>不过据我现在来看，vocab[“word”]和embedding[“word”]没啥用，毕竟vocab[“lattice”]已经是出现的所有字和词了。</p>
<h4 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h4><p>以上数据处理完成之后，就开始将数据输入到模型中进行计算，按照层级模型分为以下几个：</p>
<ul>
<li>Lattice_Transformer_SeqLabel<ul>
<li>TransformerEncoder<ul>
<li>Four_Pos_Fusion_Embedding</li>
<li>Transformer_Encoder_Layer （默认1层）<ul>
<li>MultiHead_Attention_Lattice_rel_save_gpumm</li>
<li>Positionwise_FeedForward</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Lattice-Transformer-SeqLabel"><a href="#Lattice-Transformer-SeqLabel" class="headerlink" title="Lattice_Transformer_SeqLabel"></a>Lattice_Transformer_SeqLabel</h5><p>首先传入Lattice_Transformer_SeqLabel的forward函数的参数如下：<br>lattice：size(batch_size, max_seq_len_and_lex_num)<br>bigrams：size(batch_size, max_seq_len)<br>seq_len：size(batch_size)，这一批次每个句子的长度(原始字符数量)<br>lex_num：size(batch_size)，这一批次每个句子匹配的词语个数<br>pos_s：size(batch_size, max_seq_len_and_lex_num)，开始索引<br>pos_e：size(batch_size, max_seq_len_and_lex_num)，结束索引<br>target：size(batch_size, max_seq_len)，标签序号<br>max_seq_len_and_lex_num = char_num + lex_num（这一批次加和最大的）<br>max_seq_len = char_num (这一批次句子长度最长的)</p>
<p>lattice通过lattice_embedding得到raw_embed，size为(batch_size, max_seq_len_and_lex_num, 50)<br>如果使用bigram，bigram通过bigram_embedding得到bigrams_embed，size为(batch_size, max_seq_len, 50)，因为bigrams的长度比lattice短，所以给bigrams_embed后面补零，最后size变为(batch_size, max_seq_len_and_lex_num, 50);然后将raw_embed和bigrams_embed拼接在一起：<br><code>raw_embed_char = torch.cat([raw_embed, bigrams_embed], dim=-1)</code><br>得到的raw_embed_char，size为(batch_size, max_seq_len_and_lex_num, 100);</p>
<p>如果使用bert的预训练词向量，因为bert只有单个字的词向量，所以只能为lattice中的字符部分初始化词向量bert_embed，得到的size为(batch_size, max_seq_len, 768)，虽然这里的bert_embed是根据这一批次中每个句子的实际长度(即mask过了)来初始化的，但是因为一开始的raw_embed并没有mask，所以后面还会再mask一次。得到bert_embed之后，与bigrams_embed一样也是在后面补零，最后size变为(batch_size, max_seq_len_and_lex_num, 768)。<br>然后将raw_embed_char与bert_embed拼接在一起，size为(batch_size, max_seq_len_and_lex_num, 868)。</p>
<p>现在raw_embed_char中既包含字符向量也包含词语向量，将字符向量是单独拿出来，这时候就需要知道句子的实际长度了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先过一个线性层，将size变为(batch_size, max_seq_len_and_lex_num, 160),868--&gt;160</span></span><br><span class="line">embed_char = self.char_proj(raw_embed_char)</span><br><span class="line"></span><br><span class="line"><span class="comment"># char_mask的size为(batch_size, max_seq_len_and_lex_num),但是只有句子的实际位置为True,其他位置为False</span></span><br><span class="line">char_mask = seq_len_to_mask(seq_len, max_len=max_seq_len_and_lex_num).bool()</span><br><span class="line"><span class="comment"># 得到实际的字符向量</span></span><br><span class="line">embed_char.masked_fill_(~(char_mask.unsqueeze(<span class="number">-1</span>)), <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>然后再获取词语向量，直接从最开始的raw_embed中获取<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先过一个线性层，将size变为(batch_size, max_seq_len_and_lex_num, 160),50-&gt;160</span></span><br><span class="line">embed_lex = self.lex_proj(raw_embed)</span><br><span class="line"><span class="comment"># 因为词语是在句子字符的后面，seq_len+lex_num得到每个句子实际的字符数量+词语数量</span></span><br><span class="line"><span class="comment"># mask之后与字符区域的mask取异或，就得到了纯词语区域的mask</span></span><br><span class="line">lex_mask = (seq_len_to_mask(seq_len+lex_num).bool() ^ char_mask.bool())</span><br><span class="line"><span class="comment"># 得到实际的词语向量</span></span><br><span class="line">embed_lex.masked_fill_(~(lex_mask).unsqueeze(<span class="number">-1</span>), <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>异或举例：<br>假设原始句子字符数量6，词语数量3，max_seq_len_and_lex_num=11<br>那么char_mask = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]<br>seq_len_to_mask(seq_len+lex_num)得到的是<br>lex_mask_temp = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]<br>char_mask ^ lex_mask_temp即得到：<br>lex_mask = [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</p>
<p>将字符向量和词语向量加和在一起，作为最终的输入embedding<br><code>embedding = embed_char + embed_lex</code></p>
<p>传入encoder就是TransformerEncoder，其又分为Four_Pos_Fusion_Embedding和Transformer_Encoder_Layer两个部分</p>
<p><strong>Four_Pos_Fusion_Embedding</strong><br>这里计算四种相对位置（与论文与对应）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pos_ss 计算head与head的相对位置，self.max_seq_len是所有数据中的最大句子长度</span></span><br><span class="line">pos_ss = pos_s.unsqueeze(<span class="number">-1</span>)-pos_s.unsqueeze(<span class="number">-2</span>) + self.max_seq_len</span><br><span class="line">pos_se = pos_s.unsqueeze(<span class="number">-1</span>)-pos_e.unsqueeze(<span class="number">-2</span>) + self.max_seq_len  <span class="comment"># head与tail相对位置</span></span><br><span class="line">pos_es = pos_e.unsqueeze(<span class="number">-1</span>)-pos_s.unsqueeze(<span class="number">-2</span>) + self.max_seq_len  <span class="comment"># tail与head相对位置</span></span><br><span class="line">pos_ee = pos_e.unsqueeze(<span class="number">-1</span>)-pos_e.unsqueeze(<span class="number">-2</span>) + self.max_seq_len  <span class="comment"># tail与tail的相对位置</span></span><br></pre></td></tr></table></figure></p>
<p>这里相对位置的计算结合上面提到的“重庆人和药店”举例来理解，借用flat瘦身文章的图<br><img src="/2024/11/29/nlp-flat-transformer/p2.jpg" alt><br>“重庆人和药店”加上可选词语组装成的句子，<br>开始索引为：[0, 1, 2, 3, 4, 5, 0, 2, 4]<br>结束索引为：[0, 1, 2, 3, 4, 5, 1, 5, 5]<br>左上角表格是代表head与head，以第1列为例，代表0号位置与[0, 1, 2, 3, 4, 5, 0, 2, 4]的相对位置（在其后面就是负值），所以得到第1列的值为[0, -1, -2, -3, -4, -5, 0, -2, -4]，类似地，后面每一列代表其位置与其他所有的开始索引的相对位置，就是相当于上面代码中的：<br><code>pos_s.unsqueeze(-1)-pos_s.unsqueeze(-2)</code><br>右上角表格是head与tail的相对位置，以第1列为例，代表0号位置与[0, 1, 2, 3, 4, 5, 1, 5, 5]的相对位置，所以得到第1列的值为[0, -1, -2, -3, -4, -5, -1, -5, -5]，类似地，后面每一列代表其位置与其他所有的结束索引的相对位置，就是相当于上面代码中的：<br><code>pos_s.unsqueeze(-1)-pos_e.unsqueeze(-2)</code><br>其他两个表格同理。</p>
<p>这里正好解释一下flat瘦身里提到的点，这里得到的四个相对位置size为(batch_size, max_seq_len_and_lex_num, max_seq_len_and_lex_num),后面还要再经过位置embedding层，变为(batch_size, max_seq_len_and_lex_num, max_seq_len_and_lex_num, hidden_dim)<br>这样计算下来占用的显存是非常高的（详细看flat瘦身文章就好了）<br>而仔细观察这四个相对位置编码可以发现，其实有重复的部分<br><img src="/2024/11/29/nlp-flat-transformer/p3.jpg" alt><br>就是所有的句子的字符部分，因为句子的字符部分开始索引和结束索引是相等的，所以四种相对位置计算出来也是完全相同的，按照flat-瘦身中采用的方法就是进行去重操作（V2版本中）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">pe_4 = torch.cat([pe_ss, pe_se, pe_es, pe_ee], dim=<span class="number">-1</span>)</span><br><span class="line">pe_4 = pe_4.view(<span class="number">-1</span>, <span class="number">4</span>)  <span class="comment"># size参数可能已经废弃了？</span></span><br><span class="line"><span class="comment"># pe_unique是完全不重复的张量，inverse_indices与pe_4 size一致，表示pe_4每个张量在pe_unique的位置</span></span><br><span class="line">pe_unique, inverse_indices = torch.unique(pe_4, sorted=<span class="literal">True</span>, return_inverse=<span class="literal">True</span>, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 再过位置embedding层，self.pe是一个输入2*max_seq_len, 输出为hidden_dim的Embedding层</span></span><br><span class="line">pos_unique_embedding = self.pe(pe_unique)</span><br><span class="line"><span class="comment"># pos_unique_embedding后两维是(, 4, hidden_dim)，变为(, 4*hidden_dim)</span></span><br><span class="line">pos_unique_embedding = pos_unique_embedding.view([pos_unique_embedding.size(<span class="number">0</span>), <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># self.pos_fusion_forward是一个输入为4*hidden_dim，输出为hidden_dim的线性层</span></span><br><span class="line">pos_unique_embedding_after_fusion = self.pos_fusion_forward(pos_unique_embedding)</span><br><span class="line"><span class="comment"># 使用inverse_indices再变回原来第0维的大小</span></span><br><span class="line">rel_pos_embedding = pos_unique_embedding_after_fusion[inverse_indices]</span><br><span class="line"><span class="comment"># 最终的相对位置编码(4个融合到一起)size为(batch_size, max_seq_len_and_lex_num, max_seq_len_and_lex_num, hidden_dim)</span></span><br><span class="line">rel_pos_embedding = rel_pos_embedding.view(size=[batch, max_seq_len, max_seq_len, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<p>这里的其实我没太理解这里的思路，其实最终得到的相对位置编码还是这样的size，区别就是原本有4个这种size的张量，现在只有1个，因为这里本身走了一个线性层将size缩小了，那如果我直接将原来的位置编码的hidden_dim就设置为40，不就好了？不就同样从可以缩小size嘛？（后面可以自行尝试一下）</p>
<p>另外上面在计算相对位置时，在最后统一都加了max_seq_len（最长句子的字符数），这样可以确保所有的相对位置落在区间[0, 2*max_seq_len]，因为self.pe也是计算的[0, 2*max_seq_len]这一区间的位置编码。</p>
<p>这部分操作其实对应原论文中的四种相对位置的拼接以及使用Transformer-XL中的相对位置编码将得到的相对位置(Rij)加到Attention机制中：<br><img src="/2024/11/29/nlp-flat-transformer/p4.png" alt></p>
<p>上面已经完成了四种相对位置的拼接（虽然这里是考虑到直接拼接占用显存过大，所以使用了一定的技巧）得到了Rij，这里的self.pos_fusion_forward层就是公式最外面的可学习参数W和ReLU激活<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.pos_fusion_forward = nn.Sequential(nn.Linear(self.hidden_size*<span class="number">4</span>, self.hidden_size),nn.ReLU(inplace=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure></p>
<p>那么接下来就是下一个公式的计算了<br><strong>Transformer_Encoder_Layer</strong><br>将输入和相对位置编码传入Transformer_Encoder_Layer的forward之后，接下来要传入MultiHead_Attention_Lattice_rel_save_gpumm，也就是在这一层将相对位置编码融合到Attention中。</p>
<p>输入参数有：<br>key、query、value都是之前字符向量+词语向量得到的最终embedding（Exi），<br>seq_len：这一批次中每个句子长度<br>lex_num：这一批次中每个句子匹配的词语个数<br>rel_pos_embedding：上一步骤中得到的相对位置编码</p>
<p>给query、key增加一个线性层Wq和Wk得到的是就是公式中的：WqExi、ExiWk,E（W在前在后没有区别，因为不改变size），但是这里的源代码中并没有给key增加线性层（可学习参数W），目前暂不确定原因<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.k_proj:   <span class="comment"># 源代码这里是False</span></span><br><span class="line">    <span class="keyword">if</span> self.mode[<span class="string">'debug'</span>]:</span><br><span class="line">        print_info(<span class="string">'k_proj!'</span>)</span><br><span class="line">    key = self.w_k(key)</span><br><span class="line"><span class="keyword">if</span> self.q_proj:  <span class="comment"># True</span></span><br><span class="line">    <span class="keyword">if</span> self.mode[<span class="string">'debug'</span>]:</span><br><span class="line">        print_info(<span class="string">'q_proj!'</span>)</span><br><span class="line">    query = self.w_q(query)</span><br><span class="line"><span class="keyword">if</span> self.v_proj:  <span class="comment"># True</span></span><br><span class="line">    <span class="keyword">if</span> self.mode[<span class="string">'debug'</span>]:</span><br><span class="line">        print_info(<span class="string">'v_proj!'</span>)</span><br><span class="line">    value = self.w_v(value)</span><br><span class="line"><span class="keyword">if</span> self.r_proj:  <span class="comment"># True</span></span><br><span class="line">    <span class="keyword">if</span> self.mode[<span class="string">'debug'</span>]:</span><br><span class="line">        print_info(<span class="string">'r_proj!'</span>)</span><br><span class="line">    rel_pos_embedding = self.w_r(rel_pos_embedding)</span><br></pre></td></tr></table></figure></p>
<p>注意看公式中的A和C的部分，其后面一项都是key，所以可以先将queryT和uT先加和，然后一起做运算，就可以直接得到A_C<br>公式中的u和V是啥呢<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.w_k = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line">self.w_q = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line">self.w_v = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line">self.w_r = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line">self.w_final = nn.Linear(self.hidden_size, self.hidden_size)</span><br><span class="line">self.u = nn.Parameter(torch.Tensor(self.num_heads, self.per_head_size))</span><br><span class="line">self.v = nn.Parameter(torch.Tensor(self.num_heads, self.per_head_size))</span><br></pre></td></tr></table></figure></p>
<p>计算A_C:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch * n_head * seq_len * d_head</span></span><br><span class="line">key = key.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">query = query.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">value = value.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch * n_head * d_head * key_len</span></span><br><span class="line">key = key.transpose(<span class="number">-1</span>, <span class="number">-2</span>)   <span class="comment"># 这里是把K转置了，跟公式不太一样，但是本质差不多</span></span><br><span class="line"><span class="comment"># u_for_c扩充之后，第1维和第3维与query一致</span></span><br><span class="line">u_for_c = self.u.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">-2</span>) </span><br><span class="line"></span><br><span class="line">query_and_u_for_c = query + u_for_c</span><br><span class="line"></span><br><span class="line">A_C = torch.matmul(query_and_u_for_c, key)</span><br></pre></td></tr></table></figure></p>
<p>虽然无论是转置query还是转置key，本质一样，但是queryT x key和query x keyT我认为不一样，得到得size不一样，按照公式是前者，但是代码里是后者，暂不明确这其中的原因。</p>
<p>接下来计算 B项和D项，这两项的后面都是RijWk,R，这个就是将相对位置编码过一层线性层就可以了，前面分别是queryT和VT，那么同样可以先加和，再一起计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># B</span></span><br><span class="line">rel_pos_embedding_for_b = rel_pos_embedding.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># after above, rel_pos_embedding: batch * num_head * query_len * per_head_size * key_len</span></span><br><span class="line">query_for_b = query.view([batch, self.num_heads, max_seq_len, <span class="number">1</span>, self.per_head_size])</span><br><span class="line">query_for_b_and_v_for_d = query_for_b + self.v.view(<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>, self.per_head_size)</span><br><span class="line">B_D = torch.matmul(query_for_b_and_v_for_d, rel_pos_embedding_for_b).squeeze(<span class="number">-2</span>)</span><br><span class="line"><span class="comment"># att_score: Batch * num_heads * query_len * key_len</span></span><br><span class="line">attn_score_raw = A_C + B_D</span><br></pre></td></tr></table></figure></p>
<p>这里在计算B_D时，也是转置了后一项，并没有转置query和V，很奇怪啊，搞不懂。<br>如果按照原公式来进行计算，最后得到的A_C和B_D的size不一致，而且与预期的size也有差别，所以这里应该是为了调整得到的结果size，所以与原来的公式略有差别，但是整体参与计算的每一项都是一致的。最后再补充一下关于Transformer-XL相对位置编码的介绍<br><img src="/2024/11/29/nlp-flat-transformer/p5.png" alt></p>
<p>至此所有比较核心的代码都剖析完了，剩下的代码也都比较简单易懂，详细看源代码就好了。</p>
<h3 id="实验测试"><a href="#实验测试" class="headerlink" title="实验测试"></a>实验测试</h3><p>我是先跑了Resume数据，看一下是否可以跑到论文中记录的效果，<br>Resume数据集，batch_size=10, 最长句子长度187(左右吧)，V1本地8G显存跑不了，前20轮可以跑，因为前20轮bert的预训练权重处于冻结状态，20轮之后显存就不够用了；V2进行显存瘦身之后，可以跑；<br>Resume：第45轮，测试集f1= 0.962759，论文中是：0.9586（比论文还高）</p>
<p>我的数据集：<br>因为我的数据集中的最长句子长度是276，batch_size=10，需要的显存超过了8G，本地跑不了（前20轮可以跑）</p>
<p>在4090D上运行，V2：flat+bert_unique()，最长句子长度276，句子+词语的最大长度是447，batch_size=10，占用显存14G，感觉这个占用显存大小和瘦身文章写的差别有点大，瘦身文章中写的seq_len应该是指字符数+词语数，在seq_length=300，且batch_size=10时，V2只占用4585MB，为啥seq_length只从300增加到447，就占用增加到了14G？</p>
<p>目前发现在训练自己的数据集时，在第21轮，开始调整bert的预训练权重时，准确率骤降为0。</p>
<p>尝试步骤：</p>
<ol>
<li>在第0轮直接允许bert embedding进行更新，训练第6轮时，f1还是突然降为0；</li>
<li>因为目前使用的sdg优化器，weight_decay=0，考虑是不是过拟合问题，将weight_decay调整为1e-4再次训练，在前5轮f1逐步提升，在第6开始下降，第10轮时下降为0；</li>
<li>推测是不是随机梯度下降导致无法有效学习，将优化器改为adam，weight_decay依然是1e-4，可以有效学习了，可以学习到一个比较高的f1值，但是每一轮f1值的波动会有点大，但是没有达到或者接近其他模型如(simple lexicon)可以达到的f1值；</li>
<li>使用adam优化器，然后重新在前20轮将bert embedding的更新冻结；首先从前20轮的效果来看，实际上还不如直接解冻bert embedding的效果；后面训练可以达到的最优结果是：f=0.979477, pre=0.979083, rec=0.979871。与simple lexicon达到的最优(不使用bert, f1=0.985, 使用bert, f1=0.9875)已经比较接近了；</li>
<li>因为simple lexicon没有使用bigram，而flat这里是默认使用bigram的，我认为在没有可靠的词语分割只有字符信息时，二元词可以带来一定的效果提升，但是如果已经有了可选词的情况下， 二元词的用处不大，所以我在flat中的不再使用二元词进行训练测试，达到的最优结果是：f=0.983528, pre=0.981556, rec=0.985507。相比使用Bigram，确实有约0.4%的提升；</li>
<li>在将相对位置编码融合进attention的部分，根据transformer-XL的计算公式，k_proj按照道理应该设置为True，Flat中默认为FALSE，所以设置True进行尝试发现f1只能达到0.965，所以关于k_proj的参数设置情况可能还需要进一步研究；</li>
<li>Flat使用的bert预训练权重为chinese_bert_wwm，后来有一些项目使用的bert预训练权重版本为chinese_bert_wwm_ext，所以也可以在Flat中使用ext版本来测试一下，测试结果没有明显的提升，且还有下降：f=0.979887, pre=0.9791, rec=0.980676。</li>
</ol>
<p>大模型的embedding是否可以来优化Flat，因为Flat只是把向量最后加在一起。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/NER/" rel="tag"># NER</a>
          
            <a href="/tags/flat-lattice-transformer/" rel="tag"># flat lattice transformer</a>
          
            <a href="/tags/fastNLP/" rel="tag"># fastNLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2024/06/01/spider-anhui/" rel="next" title="反爬（十）">
                <i class="fa fa-chevron-left"></i> 反爬（十）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2024/12/19/nlp-eznlp-bs/" rel="prev" title="基于boundary smoothing的中文NER">
                基于boundary smoothing的中文NER <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="chenxi">
            
              <p class="site-author-name" itemprop="name">chenxi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">116</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">213</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://docle.github.io/" title="Docle" target="_blank">Docle</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://transformerswsz.github.io/" title="Swift" target="_blank">Swift</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#flat-lattice-transformer"><span class="nav-text">flat lattice transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#论文解读"><span class="nav-text">论文解读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#环境安装"><span class="nav-text">环境安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Windows环境"><span class="nav-text">Windows环境</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#服务器环境（centos或Linux）"><span class="nav-text">服务器环境（centos或Linux）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码剖析"><span class="nav-text">代码剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据处理部分"><span class="nav-text">数据处理部分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型部分"><span class="nav-text">模型部分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Lattice-Transformer-SeqLabel"><span class="nav-text">Lattice_Transformer_SeqLabel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验测试"><span class="nav-text">实验测试</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
